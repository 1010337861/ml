{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第2章 感知机\n",
    "感知机：二分类的线性回归模型。感知机对应于输入空间中实例划分为正负两类的超平面，属判别模型。<br>\n",
    "优点：简单容易实现。<br>\n",
    "\n",
    "## 2.1 感知机模型\n",
    "定义：假设输入空间（特征空间）是$\\chi \\in R^n$,输出空间是 $y = \\{+1,-1\\}$，输入$x \\in \\chi$ 表示实例的特征向量，对应于输入空间（特征空间）的点;输出 $y \\in Y$ 表示实例的类别。<br>\n",
    "感知机：由输入空间到输出空间的函数 $$ f(x) =sign(w \\cdot x + b) (2.1)$$ ,其中$w$ 和$b$ 称为感知机参数，$w \\in R_n$ 叫做权重值（weight） 或者权重向量（weight vector）, $b \\in R$ 叫做偏置（bias） , $w \\cdot \\ x$表示内积,sign是符号函数 $$ sign(x) = \\begin{cases}+1&\\text{x $\\geq$ 0}\\\\-1&\\text{x < 0}\\end{cases} (2.2)$$\n",
    "\n",
    "感知机是线性分类模型，所判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型（linear classification model）或线性分类器（linear classifier） ，即函数集合$\\{f|f(x) = w \\cdot x +b \\} $.<br>\n",
    "几何解释：线性方程 $$ w \\cdot x + b = 0 (2.3)$$ 对应与特征空间$R_N$ 中的一个超平面$S$,其中$W$是超平面的法向量，b是超平面的截距。<br>\n",
    "超平面将特征空间划分为两个部分，位于两部分的点（特征向量）分别被分为正负两类，因此超平面$S$称为分离超平面（separating hyperplane），<br>\n",
    "<a href=\"https://imgse.com/i/piUO9zV\"><img src=\"https://z1.ax1x.com/2023/11/20/piUO9zV.png\" alt=\"piUO9zV.png\" border=\"0\" /></a><br>\n",
    "\n",
    "\n",
    "## 2.2 学习策略\n",
    "### 2.2.1 数据集的线性可分性\n",
    "数据集的线性可分：给定一个数据集 $T= \\{(x_1,y_1),(x_2,y_2),\\cdots,(x_n,y_n)\\}$,其中，$x_i \\in X \\in R_n$,$y_i \\in Y = \\{+1,-1\\} $,i = 1,2,3,...,N . 若存在某个超平面S $ w \\cdot x + b = 0$ 能够将数据集的正负实例点完全分开，则称为线性可分。\n",
    "\n",
    "### 2.2.2 感知机学习策略\n",
    "确定感知机模型参数的$w,b$ ,需要定义（经验）损失函数并将损失函数极小化。\n",
    "感知机所用的损失函数：误分类点到超平面S的总距离。任意一点 $x_0$ 到超平面的距离。 $$ {1 \\over ||w||}|w \\cdot x_0 + b|$$，这里$||W||$ 是$w$ 的$L_2$ 范数。<br>\n",
    "经验风险范数: $$L(W,b) = - \\underset {x_i \\in M} \\sum y_i(w \\cdot x_i +b) (2.4) $$ 其中M为误分类的点。<br>\n",
    "感知机学习的策略就是在假设空间中选取使损失函数（2.4）最小的模型参数w,b . 即感知机模型。\n",
    "\n",
    "## 2.3 感知机学习算法\n",
    "### 2.3.1 感知机算法的原始形式\n",
    "感知机学习算法是对以下最优化问题的算法。给定一个训练数据集 $T= \\{(x_1,y_1),(x_2,y_2),\\cdots,(x_n,y_n)\\}$,其中，$x_i \\in X \\in R_n$,$y_i \\in Y = \\{+1,-1\\} $,i = 1,2,3,...,N，求参数$w,b$使其为一下损失函数的极小化问题的解$$\\underset {w,b} min L(w,b) =- \\underset {x_i \\in M} \\sum y_i(w \\cdot x_i + b) (2.5) $$ ,其中M是误分类点的集合。<br>\n",
    "\n",
    "采用随机梯度下降法选取超参数。<br>\n",
    "1. 任意选取一个超平面$w_0,b_0$\n",
    "2. 使用梯度下降法不断地极小化目标函数。<br>\n",
    "   \n",
    "极小化的过程中不是一次是M中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。<br>\n",
    "\n",
    "假设误分类点的集合M是固定的，那么损失函数$L(w,b)$的梯度由 $$\\nabla_wL(w,b) = - \\underset {x_i \\in M} \\sum y_ix_i $$  $$\\nabla_bL(w,b) = - \\underset {x_i \\in M} \\sum y_i $$ 给出。\n",
    "\n",
    "随机选取一个误分类点$(x_i,y_i)$,对$w,b$ 进行更新；<br>\n",
    "\n",
    "$w \\larr w + \\eta y_ix_i (2.6) $ <br>\n",
    "$b \\larr b +\\eta y_i (2.7)$  \n",
    "式中$\\eta (0 < \\eta \\leq 1)$是步长，在统计学习中又称为学习率。<br>\n",
    "\n",
    "算法2.1 (感知机学习算法的原始形式)<br> \n",
    "输入：训练数据集$T={(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$ ,其中 $x_i \\in X = R^n$,$y_i \\in Y = \\{-1,+1\\},i = 1,2,\\cdots,N;$ 学习率 $\\eta(<0\\eta\\leq 1) ;$\n",
    "输出：$w,b$ ;感知机模型 $f(x) = sign(w\\cdot x + b) $<br>\n",
    "1. 选取初值$w_0,b_0$.\n",
    "2. 在训练集中选取$(x_i,y_i)$\n",
    "3. 如果$y_i(w\\cdot x_i + b) \\leq 0$ <br>\n",
    "   $w\\larr w + \\eta y_ix_i$ <br>\n",
    "   $b \\larr b +\\eta y_i $  <br>s\n",
    "4. 转至2，直至训练集中没有被误分类的点。 <br>\n",
    "\n",
    "对上述算法的解释：当一个实例点被误分类，即位于分离超平面的错误一侧，则调整$w,b$的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面的距离，直至超平面的越过改误分类点使其被争取分类。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for [1 1]: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.01, epochs=100):\n",
    "        self.weights = np.zeros(input_size + 1)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        # 添加偏置项\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        # 使用阶跃函数作为激活函数\n",
    "        return 1 if summation > 0 else 0\n",
    "\n",
    "    def train(self, training_data, labels):\n",
    "        for _ in range(self.epochs):\n",
    "            for inputs, label in zip(training_data, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                # 更新权重\n",
    "                self.weights[1:] += self.learning_rate * (label - prediction) * inputs\n",
    "                self.weights[0] += self.learning_rate * (label - prediction)\n",
    "\n",
    "# 示例用法\n",
    "# 假设输入是二维的\n",
    "training_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "# 对应的标签\n",
    "labels = np.array([0, 0, 0, 1])\n",
    "\n",
    "# 创建感知机模型\n",
    "perceptron = Perceptron(input_size=2)\n",
    "\n",
    "# 训练模型\n",
    "perceptron.train(training_data, labels)\n",
    "\n",
    "# 进行预测\n",
    "new_data = np.array([1, 1])\n",
    "prediction = perceptron.predict(new_data)\n",
    "print(\"Prediction for {}: {}\".format(new_data, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 算法的收敛性（了解）\n",
    "证明算法的收敛性：经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型。<br>\n",
    "\n",
    "前置准备：<br>\n",
    "1. 将偏置b并入权重向量$w$,记作$\\hat w = (w^T,b)^T$,将输入向量扩充，加进常数1，记作$\\hat x = (x^T,1)^T$. 这样 $\\hat x \\in R^{n+1},\\hat w \\in R^{n+1}$. 显然 $\\hat w \\cdot \\hat x = w \\cdot x +b $ . \n",
    "2.  定理（Novikoff） 设训练集$T = \\{(x_1,y_1),(x_2,y_2),\\cdots ,(x_n,y_n)\\}$ 是线性可分的，其中 $x_i \\in X = R^n  y_i  \\in Y = {-1,1} ,i = 1,2,\\cdots,N $,则\n",
    "    1.  存在满足条件$||\\hat w_{opt}|| = 1$ 的超平面 $ \\hat w_{opt} \\cdot \\hat x  = w_{opt} \\cdot + b_{opt} = 0$ 将训练数据集完全正确分开；且存在$Y>0$,对所有$i = 1,2，\\cdots ,N$, <br>\n",
    "    $$y_i(\\hat w_{opt}\\cdot\\hat x_i)  = y_i(w_{opt}\\cdot x_i + b_{opt} \\geq Y)  （2.8）$$\n",
    "    2.  令 $ R = \\underset {1 \\leq i \\leq N} {max}|| \\hat x_{i}||$,则感知机算法2.1在训练数据集上的误分类次数满足不等式， \n",
    "    $$ k\\leq \\left(  \\frac{R}{\\gamma} \\right)^2  (2.9)$$\n",
    "\n",
    "证明过程：<br>\n",
    "1. 由于训练数据集是线性可分的，按照定义2.2， 存在超平面可将训练数据集完全正确分开，取次超平面为 $ \\hat w_{opt} \\cdot \\hat x  = w_{opt} \\cdot + b_{opt} = 0$ ，$||\\hat w_{opt}|| = 1$，由于对有限个 i  = 1,2,...,N 均有\n",
    "$$y_i(\\hat w_{opt}\\cdot\\hat x_i)  = y_i(w_{opt}\\cdot x_i + b_{opt} ) > 0$$\n",
    "所以存在 \n",
    "$$ \\gamma = \\underset{t} {min}{y_i(w_{opt}\\cdot x_i +b_{opt})} $$ \n",
    ",使\n",
    "$$ y_i(\\hat w_{opt} \\cdot \\hat x_i) = y_i(w_{opt} \\cdot x_i + b_{opt}) \\geq \\gamma $$\n",
    "1. 感知机算法从$\\hat w_0 = 0 $ 开始，如果实例被误分类，则更新权重，令 $\\hat w_{k-1}$是第k个误分类之前的扩充权重向量，即 \n",
    "   $$ \\hat w_{k-1} = (w_{k-1}^t,b_{k-1})^T$$ \n",
    ",则第k个误分类实例的条件是 \n",
    "$$y_i(\\hat  w_{k-1} \\cdot \\hat x_i) = y_i(w_{k-1}\\cdot x_i +b_{k-1}) \\leq 0  (2.10)$$\n",
    "若$(x_i,y_i)$是被$\\hat w_{k-1}= (w_{k-1}^T,b_{k-1})^T误分类的数据，则w和b的更新 \n",
    "   $$ w_k \\leftarrow w_{k-1} + \\eta y_ix_i $$ \n",
    "   $$ b_k \\leftarrow b_{k-1} +\\eta y_i $$\n",
    "即 $$ \\hat w_k = \\hat w_{k-1} + \\eta y_i \\hat x_i  (2.11)$$\n",
    "\n",
    "不等式的证明：<br>\n",
    "1. $$ \\hat w_k \\cdot \\hat w_{opt} \\geq k\\eta \\gamma  (2.12)$$\n",
    "由（2.11） 及 （2.8）得 \n",
    "$$ \\hat w_k \\cdot \\hat w_{opt} = \\hat w_{k-1} \\cdot \\hat w_{opt} + \\eta y_i \\hat w_{opt} \\cdot \\hat x_i \\geq \\hat w_{k-1} \\cdot \\hat w_{opt} + \\eta \\gamma $$ \n",
    "得到不等式 （2.12） <br>\n",
    "\n",
    "2. $$||\\hat w_k||^2 \\leq k \\eta ^2 R^2  (2.13)$$\n",
    "由式（2.11）和（2.10）得\n",
    "$$||\\hat w_k||^2 = ||\\hat w_{k-1}||^2 +2 \\eta y_i \\hat w{k-1} \\cdot \\hat x_i + \\eta ^2 ||\\hat x_i||^2 \\leq ||\\hat w_{k-1}||^2 + \\eta^2 ||\\hat x_i||^2 \\leq ||\\hat w_{k-2}||^2 + \\eta^2 R^\n",
    "\\leq k \\eta^2 R^2 $$\n",
    "结合（2.12） 和 （2.13） 得\n",
    " $$ k\\leq \\left(  \\frac{R}{\\gamma} \\right)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
