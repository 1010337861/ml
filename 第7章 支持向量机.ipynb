{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第7章 支持向量机\n",
    "## 7.1 线性可分支持向量机与硬间隔最大化\n",
    "\n",
    "### 7.1.1 线性可分支持向量机\n",
    "\n",
    "给定线性可分续训练数据集，通过间隔最大化或等价得求解相应的凸二次规划问题学习得到的分离超平面为\n",
    "$$w^{*} \\cdot x + b^{*} = 0 \\tag{7.1}$$\n",
    "以及相应的分类决策函数\n",
    "$$f(x) = sign(w^{*} \\cdot x + b^{*}) \\tag{7.2}$$ \n",
    "称为线性可分支持向量机\n",
    "\n",
    "<img src=\"https://s11.ax1x.com/2023/12/29/piLWY2d.png\" alt=\"piLWY2d.png\" border=\"0\" />\n",
    "\n",
    "### 7.1.2 函数间隔和几何间隔\n",
    "函数间隔：对于给定的训练集T和超平面(w,b) 定义超平面(w,b)关于样本点 $(x_i,y_i)$的函数间隔为\n",
    "$$\\hat{\\gamma_i} = \\gamma_i(w \\cdot x_i + b)    \\tag{7.3}$$\n",
    "\n",
    "定义超平面(w,b) 关于训练集T的函数间隔为超平面(w,b)关于T中所有样本点(x_i,y_i)的函数间隔之最小值，即\n",
    "$$\\hat{\\gamma} = \\underset{i = 1,\\cdots,N}{min}\\hat{\\gamma_i} \\tag{7.4}$$\n",
    "\n",
    "函数间隔可以表示预测分类的正确性和确信度。<br>\n",
    "\n",
    "但是选择分离超平面时，只有函数间隔还不够，函数间隔会受到w和b的影响。<br>\n",
    "\n",
    "几何间隔：对于给定的训练集T和超平面(w,b) 定义超平面(w,b)关于样本点 $(x_i,y_i)$的几何间隔为\n",
    "$$\\gamma_i = \\gamma_i (\\frac{w}{||w||} \\cdot  x_i+ \\frac{b}{||w||} ) \\tag{7.5}$$\n",
    "定义超平面(w,b)关于训练集数据T的集合间隔为超平面(w,b)关于T中所有样本点$(x_i,y_i)$的几何间隔之最小值，即\n",
    "$$\\gamma = \\underset{i = 1,\\cdots,N}{min}\\gamma_i \\tag{7.6}$$\n",
    "\n",
    "超平面(w,b) 关于样本点$(x_i,y_i)$的集合间隔一般是实例点到超平面的带符号的距离，当样本点被超平面正确分类时就是超平面的据。\n",
    "\n",
    "### 7.1.3 间隔最大化\n",
    "\n",
    "间隔最大化的直观解释:对训练数据集找到几何间隔最大的超平面意味着以充分大的确信度对训练数据进行分类。 不仅将正负实例点分来，而且对难分的实例点(例平面最近的点)也有足够的确信度将他们分开。\n",
    "\n",
    "1. 最大间隔分离超平面.可表示为下述约束最优化问题：\n",
    "$$ \\underset{w,b}{max} \\gamma  \\tag{7.9}$$\n",
    "$$s.t  y_i(\\frac{w}{||w||} \\cdot x_i + \\frac{b}{||w||} ) \\geq \\gamma ,i = 1,2,\\cdots ,N \\tag{7.10} $$\n",
    "即我们希望最大化超平面(w,b)关于训练数据集的几何间隔 $\\gamma$,约束条件表示超平面(w,b)关于每个训练样本点的集合间隔至少是 $\\gamma$.\n",
    "\n",
    "和函数间隔的关系\n",
    "\n",
    "$$ \\underset{w,b}{max} \\frac{\\hat{\\gamma}}{||w||}  \\tag{7.11}$$\n",
    "$$s.t.  y_i(w \\cdot x_i + + b) \\geq \\hat{\\gamma} ,i = 1,2,\\cdots ,N \\tag{7.12} $$\n",
    "\n",
    "函数间隔改变对优化问题的不等式约束无影响，即可以 取$\\hat{\\gamma} = 1$代入上述问题。得\n",
    "$$ \\underset{w,b}{min} \\frac{1}{2}{||w||}^2 \\tag{7.13}$$\n",
    "$$s.t  y_i(w \\cdot x_i + + b) - 1 \\geq 0,i = 1,2,\\cdots ,N \\tag{7.14} $$\n",
    "\n",
    "凸优化问题：约束最优化问题\n",
    "$$ \\underset{w}{min} f(w)  \\tag{7.15} $$\n",
    "$$ s.t.   g_i(w) \\leq 0 , i = 1,2,\\cdots ,k  \\tag{7.16} $$\n",
    "$$         h_i(w) = 0,i = 1,2,\\cdots,l \\tag{7.17}  $$\n",
    "\n",
    "目标函数 $f(w)$ 和 约束函数 $g_i(w)$ 都是 $R^n$上连续可微的凸函数，约束函数$h_i(w)$是$R^n$上的仿射函数-- 一次线性函数。\n",
    "\n",
    "算法7.1 (线性可分支持向量机学习算法--最大间隔法)\n",
    "(1) 构造并求解约束最优化问题:\n",
    "$$  \\underset{w,b}{min} \\frac{1}{2}{||w||}^2  \\\\\n",
    "s.t  y_i(w \\cdot x_i + + b) - 1 \\geq 0,i = 1,2,\\cdots ,N \n",
    "$$\n",
    "\n",
    "求得最优解 $ w^{*} , b^{*} $ <br>\n",
    "(2)  由此得到分离超平面：\n",
    "$ w^{*} \\cdot x + b^{*} = 0 $ <br>\n",
    "分类决策函数 $f(x) = sign(w^{*} \\cdot x + b^{*})$\n",
    "\n",
    "2. 最大间隔分离超平面存在唯一性\n",
    "线性可分训练数据集的最大间隔分离超平面是存在且唯一的\n",
    "\n",
    "3. 支持向量和间隔边界\n",
    "\n",
    "### 7.1.4 学习的对偶算法\n",
    "拉格朗日函数： 对最优化约束问题的常用解法函数。\n",
    "引入拉格朗日乘子 $ \\alpha_i \\geq 0 ， i = 1,2 \\cdots , N$ ，定义拉格朗日函数：\n",
    "$$L(w,b,\\alpha) = \\frac{1}{2} {||w||}^2 - \\overset{N}{\\underset{i=1}{\\sum}}\\alpha_iy_i(w \\cdots x_i + b)  +  \\overset{N}{\\underset{i=1}{\\sum}}\\alpha_i \\tag{7.18} $$\n",
    "其中 $\\alpha =( \\alpha_1 ,\\alpha_2,\\cdots ,\\alpha_N)^T$ 为拉格拉日乘子。 <br>\n",
    "\n",
    "根据拉格朗日对偶性，原始问题的对偶是极大极小问题：$\\underset{\\alpha}{max} \\underset{w,b}{min}L(w,b,\\alpha)$ <br>\n",
    "\n",
    "所以为了得到对偶问题的解，需要先求$L(w,b,\\alpha)$ 对w,b的极小值，再求对$\\alpha$的极大。 <br>\n",
    "\n",
    "(1) 求$\\underset{w,b}{min}L(w,b,\\alpha)$\n",
    "将拉格朗日函数$L(w,b,\\alpha)$分别对w,b 求偏导并令其得到0. \n",
    "得 $$ w = \\overset{N}{\\underset{i=1}{\\sum}}\\alpha_iy_ix_i  \\tag{7.19}$$\n",
    "$$ \\overset{N}{\\underset{i=1}{\\sum}}\\alpha_iy_i = 0 \\tag{7.20}$$\n",
    "\n",
    "(2) 求 $\\underset{w,b}{min} L(w,b,\\alpha) $ 对$\\alpha$的极大\n",
    "\n",
    "算法7.2 （线性可分支持向量机学习算法）\n",
    "输入：线性可分训练集$T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)} ,其中x_i \\in \\chi = R^N ,y_i \\in \\Gamma ={-1,1} ,i = 1,2,\\cdots ,N$;\n",
    "输出：分离超平面和决策函数 <br>\n",
    "(1) 构造并求解约束最优化问题\n",
    "$\\underset{\\alpha}{min} \\frac{1}{2} \\overset{N}{\\underset{i=1}{\\sum}}\\overset{N}{\\underset{j=1}{\\sum}}a_ia_jy_iy_j(x_i \\cdot x_j) - \\overset{N}{\\underset{j=1}{\\sum}}a_i \\\\ s.t. \\overset{N}{\\underset{j=1}{\\sum}}a_ia_jy_i = 0 \\\\ a_i \\geq 0 i =1,2,\\cdots,N$ <br>\n",
    "\n",
    "求得最优解 $a^{*} = (a_1^{*} ,a_2^{*},\\cdots,a_N^{*})^T $\n",
    "\n",
    "(2) 计算\n",
    "$w^{*} = \\overset{N}{\\underset{i=1}{\\sum}}a_i^{*}y_ix_i$ ,并选择 $a^{*} $的一个 $a_j^{*} > 0 $ ,计算 $ b* = y_j - \\overset{N}{\\underset{i=1}{\\sum]}}a_i^{*}y_i(x) $  <br>\n",
    "\n",
    "(3) 求得分离超平面\n",
    "$w^{*} \\cdot x + b^{*} = 0$  <br>\n",
    "分类决策函数： $f(x) = sign(w^{*} \\cdot x + b^{*})$  <br> \n",
    "\n",
    "\n",
    "在线性可分支持向量机中，$ w^{*},b^{*} $ 只依赖于训练数据中对应于 $ a^{*} > 0 $ 的样本点$ (x_i,y_j) $ ,而其他样本对$ w^{*},b^{*} $ 无影响.  <br>\n",
    "\n",
    "\n",
    "## 7.2 线性支持向量机与软间隔最大化\n",
    "### 7.2.1 线性支持向量机\n",
    "线性支持向量机：对于给定的线性不可分的训练数据集，通过求解凸二次规划问题，即软间隔最大化问题 = > \n",
    "$$\\underset{w,b,\\xi}{min} \\frac{1}{2} ||w||^2 + C \\overset{N}{\\underset{i=1}{\\sum}} \\xi_i  \\tag{7.32}$$\n",
    "$$ y_i(w \\cdot x_i + b) \\leq 1- \\xi_i , i  = 1,2,...N \\tag{7.33}$$\n",
    "$$ \\xi \\leq 0 i  = 1,2,...N \\tag{7.34}$$\n",
    "\n",
    "得到的分离超平面 \n",
    "$$ w^{*} \\cdot x + b^{*} = 0 \\tag{7.35} $$ \n",
    "\n",
    "以及相应的分类决策函数 \n",
    "$$ f(x)= sign(w^{*} \\cdot x + b^{*}) \\tag{7.36} $$ \n",
    "称为线性支持向量机。\n",
    "\n",
    "### 7.2.2 学习的对偶算法\n",
    "算法7.3 （线性支持向量机学习算法）\n",
    "输入： 训练数据集 $ T= {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)},其中 ，x_i \\in \\chi = R^{N} ,y_i \\in y = {-1,1} ， i= 1,2,\\cdots,N ;$\n",
    "输出：分离超平面和分类决策函数。\n",
    "(1) 选择惩罚参数C >  0  ，构造并求解凸二次规划问题 <br>\n",
    "$\\underset{\\alpha}{min} \\frac{1}{2} \\overset{N}{\\underset{i=1}{\\sum}}\\overset{N}{\\underset{j=1}{\\sum}}a_ia_jy_iy_j(x_i \\cdot x_j) - \\overset{N}{\\underset{j=1}{\\sum}}a_i \\\\ s.t. \\overset{N}{\\underset{i=1}{\\sum}}a_ia_jy_i = 0 \\\\  0 \\leq a_i \\leq C i =1,2,\\cdots,N$ <br>\n",
    "\n",
    "求得最优解 $a^{*} = (a_1^{*} ,a_2^{*},\\cdots,a_N^{*})^T $ \n",
    "\n",
    "(2) 计算\n",
    "$w^{*} = \\overset{N}{\\underset{i=1}{\\sum}}a_i^{*}y_ix_i$ ,并选择 $a^{*} $的一个0< $a_j^{*} <C $ ,计算 $ b* = y_j - \\overset{N}{\\underset{i=1}{\\sum]}}a_i^{*}y_i(x) $  <br>\n",
    "\n",
    "(3) 求得分离超平面\n",
    "$w^{*} \\cdot x + b^{*} = 0$  <br>\n",
    "分类决策函数： $f(x) = sign(w^{*} \\cdot x + b^{*})$  <br> \n",
    "\n",
    "### 7.2.3 支持向量\n",
    "软间隔的支持向量\n",
    "\n",
    "### 7.2.4 合页损失函数\n",
    "对于线性支持向量学习来说，其模型为分离超平面 $ w^{*} \\cdot x + b^{*} = 0 $ 及决策函数 $ f(x)= sign(w^{*} \\cdot x + b^{*}) $,其学习策略为软间隔最大化，学习算法为凸二次规划。\n",
    "线性支持向量机学习还有另外一种，就是最小化一下目标函数： <br>\n",
    "$\\overset{N}{\\underset{i=1}{\\sum}}[1-y_i(w \\cdot x_i + b)]_+  + \\lambda||w||^2  \\tag{7.57}$\n",
    "\n",
    "目标函数的第一项是经验损失或经验风险，函数\n",
    "$$L(y(w \\cdot x + b)) = [1-y(w \\cdot x + b)]_+  \\tag{7.58}$$ \n",
    "称为合页损失函数，下标“+”表示一下取正直的函数。\n",
    "\n",
    "\n",
    "## 7.3 非线性支持向量机与核函数\n",
    "### 7.3.1 核技巧\n",
    "用线性分类方法求解\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
