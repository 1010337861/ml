{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提升方法\n",
    "提升（boosting）方法是一种常用统计方法，在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类性能。\n",
    "\n",
    "## 8.1 提升方法AdaBoost算法\n",
    "### 8.1.1 提升方法的基本思路\n",
    "\n",
    "提升方法的基本思想：对于复杂任务来说，将多个专家判断进行适当综合所得得出判断，要比其中任何一个专家单独的判断好。\n",
    "\n",
    "强可学习（strongly learnable）：在概率近似正确（probably approximately correct ,PAC）学习框架中，一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么称这个概念是强可学习的<br> \n",
    "弱科学系（weakly learnable） ：如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好，那么称这个概念是弱可学习的。 <br>\n",
    "\n",
    "在PAC学习的框架下，一个概念是强可学习的充分必要条件是这个概念是弱可学习的。 <br>\n",
    "\n",
    "对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则（弱分类器）要比求精确的分类规则（强分类器）容易的多。<br>\n",
    "\n",
    "提升方法就是从弱学习算法出发，反复学习，得到一系列弱分类器（又称为基本分类器），然后组合这些弱分类器，构成强分类器。大多数的提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。<br>\n",
    "\n",
    "提升方法要回答的两个问题：\n",
    "1. 在每一轮如果改变训练数据的权重或概率分布？ -- 方法：提高那些被前一轮弱分类器错误分类样本的权值，降低那些被正确分类样本的权值。目的：让没有得到正确分类的数据由于权值的提高而受到后一轮弱分类器的更大关注。\n",
    "2. 如何将弱分类器组合成一个强分类器？ -- 方法：采取加权多说表决的方法，具体的，减少分类误差率大的弱分类器的权值，使其在表决中起较小的作用。\n",
    "\n",
    "### 8.1.2 AdaBoost算法\n",
    "\n",
    "Adaboost算法：假设给定一个二分类的训练数据集 \n",
    "$$T= \\{(x_1,y_1),(x_2,y_2), \\cdots,(x_N,y_N)\\} $$\n",
    "其中，每个样本点由实例和标记组成。 实例 $x_i \\in X  \\subseteq R^n$ ,标记$y_i \\in Y = {-1,+1}$ X是实例空间，Y是标记集合。 <br>\n",
    "AdaBoost 利用一下算法，从训练数据中学习一系列弱分类器或基本分类器，并将这些分类器组合成一个强分类器。<br>\n",
    "\n",
    "算法 8.1（AdaBoost） \n",
    "输入：训练数据集 $ T= \\{(x_1,y_1),(x_2,y_2), \\cdots,(x_N,y_N)\\} $ ,其中$x_ \\in X  \\subseteq R^n ，y_i \\in Y = {-1,+1}$ ；弱学习算法；\n",
    "输出：最终分类器G(x).\n",
    "1. 初始化训练数据的权值分布 \n",
    "$$D_1 = (W_{11},\\cdots,W_{1i},\\cdots,W_{1N}) ,W_{1i} = \\frac{1}{N},i = 1,2,\\cdots,N $$\n",
    "2. 对$m  = 1,2,\\cdots,M$\n",
    "   1. 使用具有权值分布$D_M$的训练数据集学习，得到基本分类器 $$G_m(x):X -> {-1,+1}$$\n",
    "   2. 计算$G_m(x)$ 在训练数据集上的分类误差率 $$e_m =P(G_m(x_i) \\neq y_i) = \\sum_{i=1}^{N} W_{mi}I(G_m(x_i) \\neq y_i )  \\tag {8.1} $$\n",
    "   3. 计算$G_m(x)$ 的系数 $$\\alpha_m = \\frac{1}{2} log \\frac{1-e_m}{e_m} \\tag{8.2}$$ , 这里的对数是自然对数。\n",
    "   4. 更新训练数据集的权值分布 \n",
    "   $$ D_{m+1} = (W_{m+1,1},\\cdots,W_{m+1,i},\\cdots,W_{m+1,N})  \\tag{8.3} $$\n",
    "   $$W_{m+1,i} = \\frac{W_{mi}}{Z_{m}}exp(-\\alpha_my_iG_m(x_i)) , i = 1,2,\\cdots,N  \\tag{8.4} $$\n",
    "   这里，$ Z_m $ 是规范化因子 \n",
    "   $$ Z_m = \\sum_{i=1}^{N} W_{mi} exp(-\\alpha_my_iG_m(x_i))  \\tag{8.5} $$\n",
    "   它使$ D_{m+1} $ 称为一个概率分布。\n",
    "\n",
    "3. 构成基本分类器的线性组合 $$ f(x) = \\sum_{m=1}^{m} \\alpha_{m}G_m(x)  \\tag{8.6} $$ ,得到最终分类器。\n",
    "$$G(x)  =sign(f(x)) = sign{\\sum_{m=1}^{m} \\alpha_{m}G_m(x) } \\tag{8.7} $$\n",
    "\n",
    "\n",
    "## 8.2 AdaBoost 算法的训练误差分析\n",
    "AdaBoost 最基本的性质是它能在学习过程中不断减少训练误差，即在训练误差数据集上的分类误差率。\n",
    "定理 8.1 (AdaBoost 的训练误差界) AdaBoost 算法最终分类器的训练误差界为 $$\\frac{1}{N} \\sum_{i=1}^{N} I(G(x_i) \\neq y_i) \\leq  \\frac{1}{N}\\sum_{i} exp(-y_if(x_i)) =  \\prod_{m}Z_m  \\tag{8.9}$$\n",
    "这里，$ G(X) ,f(x) 和Z_m 分别由式(8.7),式(8.6) 和式(8.5)给出 $  <br>\n",
    "\n",
    "定理 8.2 (二分类问题AdaBoost的训练误差界)\n",
    "$$\\prod_{m=1}{M} Z_{m} = \\prod_{m=1}{M} [2\\sqrt{e_m(1-e_m)}] = \\prod_{m=1}{M} \\sqrt{1-4\\gamma_m^2} \\leq exp(-2\\sum_{m=1}^{M} \\gamma_m^2) \\tag{8.10} $$\n",
    "\n",
    "这里，$\\gamma_m = \\frac{1}{2} -e_m $.\n",
    "\n",
    "\n",
    "## 8.3 AdaBoost算法的解释\n",
    "可以任务AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分布算法是的二类分类学习方法。\n",
    "### 8.3.1 前向分布算法\n",
    "考虑加法模型（additive model） \n",
    "$$f(x) = \\sum_{m=1}^{M}\\beta_{m}b(x;\\gamma_m) \\tag{8.13}$$\n",
    "其中，$ b(x;\\gamma_m) $为基函数，$\\gamma_m$为基函数的参数，$\\beta_m$ 为基函数的系数。 <br>\n",
    "在给定训练数据及损失函数$L(y,f(x))$的条件下,学习加法模型$f(x)$ 成为经验风险极小化即损失函数极小化问题：\n",
    "$$\\underset{min}{\\beta_m,y_m} \\sum_{i=1}^{N} L(y_i, \\sum_{m=1}{M}\\beta_{m}b(x_i;\\gamma_m))  \\tag{8.14}$$ \n",
    "\n",
    "算法8.2 (前向分布算法)\n",
    "\n",
    "输入：训练数据集 $ T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)} $ ;损失函数L(y,f(x));基本函数{b(x:\\gamma)}$; <br>\n",
    "输出：加法模型$f(x)$ <br>\n",
    "\n",
    "(1) 初始化$f_0(x) = 0$   <br>\n",
    "(2) 对$m = 1,2,\\cdots ,M $ <br>\n",
    "(a) 极小化损失函数\n",
    "$$(\\beta_{m},Y_{m}) = arg\\underset{min}{\\beta,Y} min_{i=1}^{N}L(y_i,f_{m-1}(x_i)+\\beta b(x_i;\\gamma))  \\tag{8.16}$$ \n",
    "得到参数 $\\beta_{m},\\gamma_{m}$  <br>\n",
    "\n",
    "(b) 更新\n",
    "$$ f_m(x) = f_{m-1}(x) +  \\beta b(x_i;\\gamma) \\tag{8.17}$$\n",
    "\n",
    "(3) 得到加法模型\n",
    "$$f(x) = f_M(x) = min_{m=1}^{M}\\beta_m b(x;Y_m) \\tag{8.18}$$\n",
    "\n",
    "这样，前向分布算法将同时求解从m=1 到 M所有参数 $\\beta_m ,\\gamma_m $ 的优化问题简化为逐次求解各个  $\\beta_m ,\\gamma_m $  的优化问题。\n",
    "\n",
    "### 8.3.2 前向分布算法与AdaBoost\n",
    "待理解\n",
    "\n",
    "\n",
    "## 8.4 提升树\n",
    "提升树是以分类树或回归树为基本分类器的提升方法，提升树被认为是统计学习中性能最好的方法之一。\n",
    "\n",
    "### 8.4.1 提升树模型\n",
    "提升方法实际采用加法模型（即基函数的线性组合）与前向分布算法。以决策树为基函数的提升方法称为提升树（boosting tree）. \n",
    "提升树模型可表示为决策树的加法模型：\n",
    "$$f_M(x) = \\sum_{m=1}^{M} T(X; \\theta_m) \\tag{8.23}$$\n",
    "\n",
    "### 8.4.2 提升树算法\n",
    "提升树算法采用前向分布算法。首先确定初始提升树$f_0(x) = 0 $,第m步的模型是 \n",
    "$$ f_m(x) = f_{m-1}(x) + T(x;\\theta_m)  \\tag{8.24}$$ \n",
    "\n",
    "其中，$f_{m-1}为当前模型，通过经验风险极小化确定下一颗决策树的参数 $\\theta_m $.<br>\n",
    "$$\\hat{\\theta}_m = arg min_{\\theta_m} \\sum_{i=1}^{N} L(y_i,f_{m-1}(x_i) + T(x_i;\\theta_m)) \\tag{8.25} $$\n",
    "\n",
    "优点：提升树是一个高功能的学习算法，由于树的线性组合可以很好的拟合数据，即使数据中的输入与输出之间的关系很复杂也是如此。<br>\n",
    "\n",
    "回归问题提升树。\n",
    "已知一个训练数据$T={(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)} ,x_i \\in \\chi \\subset R^N$ ,$\\chi$ 为输入空间， $y_i \\in \\gamma \\subset R$, $\\gamma$为输出空间。<br>\n",
    "如果将输入空间$\\chi$划分为J个互不相交的区域 $R_1,R_2,\\cdots,R_j$ ,并且在每个区域上确定输出的常亮$c_j$,那么树可表示为\n",
    "$$ T(x;\\theta) = \\sum_{j=1}^{j}C_j I(x \\in R_j) \\tag{8.26}$$\n",
    "\n",
    "其中，参数$\\theta = {(R_1,c_1),(R_2,c_2),\\cdots,(R_j,c_j)}$  表示树的区域划分和各区域上的场数，J是回归树的复杂度即叶结点个数。\n",
    "\n",
    "算法：8.3（回归问题的提升树算法）\n",
    "输入： $T={(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)} ,x_i \\in \\chi \\subset R^N$ ,$\\chi$ ， $y_i \\in \\gamma \\subset R$ 。<br>\n",
    "输出：提升树$f_M(x) $<br>\n",
    "\n",
    "1. 初始化$f_0(x) = 0$ \n",
    "2. 对$m = 1,2,\\cdots,M$\n",
    "   1. 计算残差 $$ r_{mi} = y_i = f_{m-1}(x_i) ,i =1,2,\\cdots,N $$\n",
    "   2. 拟合残差$r_{mi} 学习一个回归树，得到T(x;\\theta_m)$\n",
    "   3. 更新$f_m(x) = f_{m_1}(x) + T(x;\\theta_m)$\n",
    "3. 得到回归问题提升树  $$f_M(x) = \\sum_{m=1}^{M} T(x;\\theta_m) $$\n",
    "   \n",
    "### 8.4.3  梯度提升\n",
    "算法：8.4 (梯度提升算法)\n",
    "输入： $T={(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)} ,x_i \\in \\chi \\subset R^N$ ,$\\chi$ ， $y_i \\in \\gamma \\subset R $ ;损失函数$L(y,f(x))$。<br>\n",
    "输出：回归树 $\\hat{f} (x)$ <br>\n",
    "1. 初始化 $$f_0(x) = arg \\min_c \\sum_{i=1}^{N}L(y_i,c) $$\n",
    "2. 对 $m=1,2,\\cdots$ ,M\n",
    "   1. 对$i= 1,2,\\cdots ,N $ 计算 $r_{mi} = -[\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)}]_{f(x) = f_{m=1}(x)}$\n",
    "   2. 对$r_{mi}拟合一个回归树，得到第m棵树的叶结点区域$R_{mj}$ ,j = 1,2 ,\\cdots ,j$\n",
    "   3. 对$j = 1,2 ,\\cdots ,j$ 计算 $C_{mj} = arg \\min_c \\sum_{x_i \\in R_{mj}}L(y_i,f_{m-1}(x_i)) + c $\n",
    "   4. 更新$f_m(x) = f_{m-1}(x) + \\sum_{j=1}^{j}c_{mj}I(x \\in R_{mj})$\n",
    "3. 得到回归树 $\\hat{f}(x) = f_M(x) = \\sum_{m=1}^{M} \\sum_{j=1}{J} C_{mj}I(x\\in R_{mj}) $\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
