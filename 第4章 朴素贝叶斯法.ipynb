{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4章 朴素贝叶斯法\n",
    "\n",
    "朴素贝叶斯（navie Bayes）法是基于贝叶斯定理与特征条件独立假设的分类方法。<br>\n",
    "对于给定训练集，首先基于特征条件独立假设学习输入/输出的联合概率分布；然后基于模型，对于给定的输入x，利用贝叶斯定理求出后验概率最大的输出y.\n",
    "\n",
    "## 4.1 朴素贝叶斯法的学习与分类\n",
    "### 4.1.1 基本方法\n",
    "设输入空间 $ \\chi \\subseteq  R^N $ 为n维向量的集合，输出空间位类标记集合 $\\gamma = {c_1,c_2,\\cdots,c_k}$.输入为特征向量 $ x \\in \\Chi $,输出为类标记 （class label） $ y \\in \\gamma $,X是定义在输入空间$\\Chi$上的随机向量，Y 是定义在输出空间\\gamma$上的随机变量。 $P(X,Y) 是X和Y$ 的联合概率分布. 训练集 $ T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,Y_N)} 由P(X,Y)独立同分布产生。<br>\n",
    "\n",
    "朴素贝叶斯法通过训练数据集学习联合概率分布P(X,Y). 学习先验概率分布及条件概率分布。 先验概率分布 $$P (Y = c_k) k = 1,2,\\cdots ,K (4.1)$$\n",
    "\n",
    "条件概率分布 $$ P(X = x |Y= c_k) = P(X^{(1)} = x^{(1)},\\cdots,X^{(n)}=x^{(n)}|Y=c_k),k = 1,2,\\cdots ,K (4.2)$$\n",
    "得到联合概率分布P(X,Y) <br>\n",
    "\n",
    "条件独立性假设： $$ P(X=x | Y = c_k) = P(X^{(1)} = x^{(1)},\\cdots , X^{(n)} = x^{(n)} | Y=c_k) = \\prod_{j=1}^n P(X^{(j)} = x^{(j)}|Y=c_k) (4.3) $$\n",
    "\n",
    "条件独立性假设等于假设分类特征在类确定的条件下都是条件独立的。 这样假设会使算法简单，但会牺牲一定的分类准确率。<br>\n",
    "\n",
    "贝叶斯分类时，对给定的输入x, 通过学习到的模型计算后验概率分布 $P(Y= c_k|X=x)$ ,将后验概率最大的类作为x的类输出。<br>\n",
    "\n",
    "后验概率的计算 -- 根据贝叶斯定理: $$ P=(Y=c_k| X=x) = \\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum_k P(X=x|Y=c_k)P(Y=c_k)} $$\n",
    "\n",
    "将式（4.3）代入（4.2） 得 $$P(Y=c_k|X=x) = \\frac{P(Y=c_k) \\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum_k p(Y=c_k)  \\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)}  k = 1,2,\\cdots , K (4.5)$$\n",
    "\n",
    "朴素贝叶斯分类器： $$ y =\\underset {c_k}{argmax} P(Y=c_k) \\prod_j P(X^{(j)}=x^(j)|Y=c_k)    (4.6)$$\n",
    "\n",
    "\n",
    "### 4.1.2 后验概率最大化的含义\n",
    "\n",
    "后验概率最大化等价于期望风险最小化.<br>\n",
    "假设损失函数为0-1函数 ： $$ L(x,,f(X)) = \\begin {cases} 1 ,Y \\neq f(X) \\\\   0 ,Y = f(X) \\end {cases} $$ ,$f(X)$是分类决策函数。<br>\n",
    "\n",
    "这是期望风险函数为 $$ R_{exp}(f) = E[L(Y,F(X))]$$  \n",
    "\n",
    "期望是对联合分布$P(X,Y)$，取得，由此条件期望 $$ R_{exp}(f) = E_X \\sum_{K=1}^K [L((c_k),f(X))]P(c_k|X)$$\n",
    "\n",
    "\n",
    "使期望风险函数最小化，只需对X=x 逐个极小化，由此得到： $$ f(x) = arg \\underset {y \\in \\gamma} {min} \\sum_{k = 1} ^{n} L(c_k,y)P(c_k|X=x) \n",
    "\\\\ = arg min_{y \\in \\gamma} \\sum_{k=1}^{K} = P(y \\neq c_k|X=x) \n",
    "\\\\ = arg min_{y \\in \\gamma}(1-P(y=c_k|X=x))\n",
    "\\\\ = arg max_{y \\in \\gamma } P(y=c_k |X=x) $$   \n",
    "\n",
    "这样期望风险最小化最变成后验概率最大化. 即朴素贝叶斯原理. \n",
    "\n",
    "\n",
    "## 4.2 朴素贝叶斯法的参数估计\n",
    "\n",
    "### 4.2.1 极大似然估计\n",
    "贝叶斯算法中，学习意味着估计 $P(Y= c_k) 和 P(X^{(j)} = x^{(j)}| Y=c_k)$ <br>\n",
    "可以应用极大似然估计法估计相应概率.<br>\n",
    "[极大似然估计](https://baike.baidu.com/item/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/3350286)<br>\n",
    "\n",
    "先验概率$P(Y=c_k)$ 的极大似然估计： $$P(Y=c_k) = \\frac{ \\underset {i=1} \\sum ^N I(y_i = c_k)}{N} , k = 1,2,\\cdots,k  \\tag((4.8)$$\n",
    "\n",
    "设第j个特征$x^{(j)}$可能取值的集合为${a_{j1},a_{j1},\\cdots ,a_{js_j}}$,条件概率 $P(X^{(j)} = a_{jl} |Y = c_k) $ 的极大似然估计是\n",
    "$$P(X^{(j)} = a_{jl} |Y = c_k) = \\frac{ \\underset {i=1} \\sum ^N I(x_i^{(j)} = a_{(jl)},y_j = c_k)}{ \\underset {i=1} \\sum ^N I(y_i = c_k)}   j = 1,2,\\cdots , n ; l = 1,2,\\cdots ,S_j ; k = 1,2,\\cdots , K \\tag((4.9)$$\n",
    "$x_i^{(j)}$ 是第i个样本的第j个特征；$a_{jl}$是第j个特征; $ajl$是j个特征可能取的第l个值；I是指示函数。\n",
    "\n",
    "\n",
    "### 4.2.2 学习与分类算法\n",
    "输入:训练数据$T={(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$ , \n",
    "输出：x的分类\n",
    "(1) 计算先验概率及条件概率 $$ P(Y=c_k) =\\frac{ \\sum_{{i = 1}}^{N} I(y_i=c_k)} {N} ,k = 1,2,\\cdots,k $$\n",
    " $$ P(X^{(j)}=a_(jl)|Y=c_k) = \\frac{ \\sum_{i = 1}^{N} I(x_i^{(j)}=a_{jl},y_i = c_k)} {\\sum_{{i = 1}}^{N} I(y_i=c_k)} j = 1,2,\\cdots ,n; l = 1,2,\\cdots,S_j; k=1,2,\\cdots, K $$\n",
    "\n",
    " (2) 对给定的实例$ x  = (x^{(1)},x^{(2)},\\cdots,x^{(n)})^T $ ,计算 \n",
    "    $$ P = (Y = c_k) \\prod_{j = 1} ^{n} P(X^{(j)}=x^{(j)} |Y = c_k) , k = 1,2,\\cdots,K $$ \n",
    "\n",
    "(3) 确定 实例x 的类\n",
    "    $$ y =arg \\underset {c_k}{max} P(Y =c_k) \\prod_{j = 1} ^{n} P(X^{(j)}=x^{(j)} |Y = c_k) $$\n",
    "\n",
    "### 4.2.3 贝叶斯估计\n",
    "避免使用极大似然估计出现概率值为0的情况。 \n",
    "条件概率的贝叶斯估计： \n",
    "$$ p_{\\lambda} = (X^{(j)} = a_{jl}| Y = C_k) =  \\frac {\\sum_{i = 1}^{N} I(X_i^{(j)} = a_{jl},y_i= C_k) + \\lambda)}{\\sum_{i = 1}^(N)I(y_i = c_k) +S_j\\lambda}   \\tag {4.10} $$ \n",
    "\n",
    "当 $\\lambda \\geq 0$ 时 相当于在随机变量各取值的频数赋予一个正数 $\\lambda \\geq 0 $ . 当 $ \\lambda = 0 $时，极大似然估计； $ \\lambda = 1 $ 拉普拉斯平滑。 <br>\n",
    "对任何的$ l = 1,2,\\cdots ,S_j , k = 1,2,\\cdots ,K $ 有 \n",
    "    $$ P_{\\lambda}(X^{(j)} = a_{jl} = Y = c_k) \\geq 0 \\\\  \\sum_{i = 1}^{s_j} P(x^{(j)} = a_{jl}|Y = c_k) = 1 $$\n",
    "\n",
    "先验概率的贝叶斯估计 $$P_{\\lambda} = \\frac{\\sum_{i = 1}^{N} I(y_i = c_k) + \\lambda}{ N + K \\lambda }  \\tag {4.11} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
