{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a herf='https://blog.csdn.net/weixin_42546496/article/details/88115095'>常用公式</a>\n",
    "\n",
    "# 1\n",
    "## 1.3 统计学习的三要素 \n",
    "方法 = 模型 + 策略 + 算法\n",
    "\n",
    "### 1.3.1  模型：\n",
    "1. 监督过程：所要学习的条件概率分布或决策函数。模型的假设空间（hypothesis space）版号所有可能条件概率分布或决策函数。\n",
    "\n",
    "### 1.3.2 策略\n",
    "1. 按照什么样准则学习或选择最优的模型。统计学习的目标在于从假设控价选取最优的模型。\n",
    "\n",
    "##### 1 损失函数和风险系数\n",
    "1. 监督问题中是在假设空间 $F$ 中选取模型 $f$ 作为决策函数，对于给定的输入$X$,由$f(X)$ 给出相应的的输出$Y$, 这个输出的预测值$f(X)$可能和真实值$Y$不一致，用一个损坏函数（loss function）或代价函数（cost function）来度量预测错误的程度。损失函数是$f(X)$ 和Y的非负实值函数，记作$L(Y,f(X))$ .\n",
    "2. 常用损失函数：\n",
    "   1. 0- 1 损失函数\n",
    "   2. 平方损失函数\n",
    "   3. 绝对损失函数\n",
    "   4. 对数损失函数\n",
    "损失函数值越小，模型就越好，由于模型的输入，输出$(X,Y)$ 是随机变量，遵循联合分布$P(X,Y)$ ,所以损失函数的期望：\n",
    " <br><center> $R_{exp}(f) = E_{p}[L(Y,F(X))] = \\int_{x*y}L(y,f(x))P(x,y)dxdy$   (1.9)  </center><br>\n",
    "  -- 》 理论上模型$f(x)$关于联合分布$P(X,Y)的平均意义下的损失，称为风险系数（risk function）或者期望损失（expected loss）。\n",
    "\n",
    "故学习的目标就是选择期望损失最小的模型。\n",
    "给定一个训练数据集：$T = \\{(x_1,y_1),(x_2,y_2),...,(x_N,y_n)\\}$ \n",
    "\n",
    "模型 $f(X)$ 关于训练数据集的平均损失函数称为经验风险（empirical risk）或经验损失（empirical loss） ，记作 $R_{emp}$: \n",
    " $$R_{emp}(f) = {1\\over N}\\sum_{i=1}^nL(y_i,f(x_i))$$   (1.10) \n",
    "\n",
    "\n",
    "##### 2. 经验风险最小化与结构风险最小化。\n",
    "经验风险最小化（empirical risk minimization ,ERM）求最优模型就是求解最优化问题：\n",
    "<br> <center> $$\\underset {f \\epsilon F} {min} {1 \\over N}\\sum_{i=1}^nL(y_i,f(x_i))$$  (1.11)</center><br>  \n",
    "其中$F$ 是假设空间。当样本容量足够大的时候经验风险最小化能够保证很好的学习效果。如极大似然估计（maximun likelihood estimation） 就是风险最小化的一个例子。\n",
    "\n",
    "结构风险最小化（strictircal risk minimization ,SRM）就是为了防止过拟合而提出的策略，结构风险最小化等价于正则化（regularization）。结构风险在经验风险加上表示模型复杂度的正则化项（regularizer）或惩罚项（penalty term）。再假设空间、损失函数以及训练数据集确定的情况下，结构风险的定义就是：\n",
    "<br><center> $$R_{srm}(f) = {1 \\over N} \\sum_{i=1}^NL(y_i,f(x_i)) + \\lambda J(f)$$   (1.12)   </center><br>               \n",
    "其中$J(f)为模型的复杂度，是定义在假设空间$F$上的泛函，模型$f$越复杂，复杂度$J(f)$就越大；反之模型$f$越简单，复杂度$J(f)$就越小，即复杂度表示了对复杂模型的惩罚，$\\lambda \\geq$ 0是系数，用以权衡经验风险和模型复杂度。结构风险小需要经验风险金和模型复杂度同时小。结构风险小的模型往往对训练数据和未知的测试数据都有较好的预测。\n",
    "\n",
    "\n",
    "结构风险最小化的策略认为结构风险最小的模型就是最优的模型，所以求最优模型就是求解最优化问题：\n",
    "<br><center> $$ \\underset {f \\epsilon F} {min} {1 \\over N} \\sum_{i=1}^nL(y_i,f(x_i)) + \\lambda J(f) $$  (1.13)  </center></br>\n",
    "\n",
    "\n",
    "监督问题 --》 风险函数或结构函数的最优化问题（1.11）和（1.13）。此时经验或结构风险函数就是最优化的目标函数。\n",
    "\n",
    "### 1.3.3 算法\n",
    "算法：学习模型的具体计算法。\n",
    "\n",
    "\n",
    "\n",
    "## 1.4 模型评估和模型选择\n",
    "\n",
    "### 1.4.1 训练误差和测试误差\n",
    "统计学习的目的：使学到的模型对已知和未知的数据都有很好的预测能力。\n",
    "当损失函数给定时，评估标准为基于损失函数模型的训练误差（training error）和模型的测试误差(test error) \n",
    "注：统计学习方法具体采用的损失函数未必是评估时使用的损失函数。\n",
    "假设学习到的模型的是 $Y=\\hat{f}(X)$ ,训练误差是模型$Y=\\hat{f}(X)$ 关于训练数据集的平均损失：\n",
    "$$R_{emp}(\\hat{f}) = {1 \\over N} \\sum_{i=1}^nL(y_i,\\hat{f}(x_i))     (1.14) $$ \n",
    "其中N为训练样本容量。\n",
    "\n",
    "测试误差是模型$Y = \\hat{f}(X)$关于测试数据集的平均损失：$$e_{test} = {1 \\over {N'}} \\sum_{i=1}^{N'}L(y_i,\\hat{f}(x_i))    (1.15)$$ \n",
    "\n",
    "例如，当损失函数是0-1 损失时，测试误差就变成了常见的测试数据集上的误差率（error rate）$$ e_{test} =  {1 \\over {N'}} \\sum_{i=1}^{N'}I(y_i \\neq \\hat{f}(x_i) ) (1.16)$$\n",
    "\n",
    "I表示指示函数（indicator function） ，即$y \\neq \\hat{f}(x)$时为1，否则为0.\n",
    "相应的，常见测试数据上的准确率（accuracy） 为 $$r_{test} = {1 \\over {N'}} \\sum_{i=1}^{N'}I(y_i = \\hat{f}(x_i) ) (1.17)$$\n",
    "\n",
    "$$r_{rest} + e_{test} = 1$$\n",
    "\n",
    "训练误差的大小：对判断给定问题是不是容易学习是有意义的，但本质上不重要。\n",
    "测试误差反映了学习方法对未知测试数据集的预测能力。\n",
    "\n",
    "显然给定两种学习方法，测试误差小的方法具有更好的预测能力，是更有效的方法，通常将学习方法对未知数据的预测能力称为泛化能力（generalization ability ）\n",
    "\n",
    "### 1.4.2 过拟合与模型选择\n",
    "模型选择（mode selection）：当假设空间含有不同复杂度（例如，不同参数个数）的模型时。希望选择和学习一个合适的模型，如果假设空间存在“真”模型,那么所选的模型应该逼近真模型。\n",
    "即：所选的模型要与真模型的参数个数相同，所选的模型的参数向量与真模型的参数向量相近。\n",
    "\n",
    "过拟合（over-fitting）：一味追求提高对训练数据的预测能力，所选模型的复杂度真模型更高。学习时的选择的模型所包含的参数过度。\n",
    "\n",
    "## 1.5 正则化与交叉验证\n",
    "\n",
    "### 1.5.1 正则化\n",
    "正则化（regularization）：结构风险最小化策略的实现，是在经验风险上加上一个正则化项（regularzer）或惩罚项（penalty term）。<br>\n",
    "正则化一般为模型复杂度的**单调递增函数** ，模型越复杂，正则化值就越大。<br>\n",
    "比如，正则化项可以是模型参数向量的范数。\n",
    "正则化一般具有如下形式：$$ \\underset {f \\epsilon F} {min} {1 \\over N} \\sum_{i=i}^NL(y_i,f(x_i))+\\lambda J(f)  (1.19)$$\n",
    "\n",
    "其中第一项为经验风险，第2项是正则化项，$\\lambda \\geq 0$ 为调整两者之前的关系系数。<br>\n",
    "\n",
    "正则化：\n",
    "1. 参数向量的$L_2$范数：$$ L(W) = {1 \\over N} \\sum_{i=i}^N(f(x_i;w)-y_i)^2+{\\lambda \\over 2} ||w||^2$$ ,$||w||$ 表示参数向量$w$的$L_2$范数。\n",
    "2. 参数向量的$L_1$范数：$$L(W) = {1 \\over N} \\sum_{i=i}^N(f(x_i;w)-y_i)^2+\\lambda  ||w||_1 $$ 这里的||w||_1表示参数向量$w$的$L_1$范数。\n",
    "\n",
    "正则化的作用：选择经验风险和模型复杂度同时比较小的模型。\n",
    "\n",
    "### 1.5.2 交叉验证\n",
    "交叉验证（cross validation）\n",
    "\n",
    "将给定样本随机切分成三个部分：训练集（training set） 、验证集（validation set） 和测试集（test set）。\n",
    "\n",
    "1. 简单交叉验证\n",
    "   随机将样本数据分成两部分。一部分作为训练集，一部分作为测试（常用70%数据为训练集，30%数据为测试集。）然后用训练集在各种条件下（例如，不同的参数个数）训练模型，从而得到不同的模型，在测试集评价各个模型的测试误差，选出误差最小的模型。\n",
    "2. S折交叉验证：应用最多\n",
    "   步骤：1. 将已给数据切分为S个互不相交的大小相同的自己；2. 利用S-1个子集的数据训练模型;3. 利用余下子集测试模型。将上述过程重复S次 选出最优解。\n",
    "3. 留一交叉验证\n",
    "   当S=N时，称为留一交叉验证（leave-one-out cross validation ） ，常用于数据缺乏时，这里N为给定数据集容量。\n",
    "\n",
    "## 1.6 泛化能力\n",
    "### 1.6.1 泛化误差\n",
    "泛化能力：由该方法学习到的模型对未知数据的预测能力。<br>\n",
    "泛化误差：如果学习到的模型是 $\\hat{f}$ 那么用这个模型对未知数据预测的误差即为泛化误差（generalization error）$$R_{exp}\\hat{f} = E_p[L(Y,\\hat(f)(X))]=\\int_{x*y} L(y,\\hat{f}(x))P(x,y)dxdy    (1.20) $$\n",
    "\n",
    "泛化误差：就是所学习到的模型的期望风险，反应了学习方法的泛化能力。\n",
    "\n",
    "### 1.6.2 泛化误差上界\n",
    "衡量学习方法的泛化能力的指标。泛化误差上界(generalizaiton error bound) <br>\n",
    "具有的性质：\n",
    "1. 它是样本容量的函数，样本容量增加时，泛化上界趋于0\n",
    "2. 它是假设空间容量的函数，假设空间容量越大，模型越难学，误差上界就越大。<br>\n",
    "\n",
    "\n",
    "\n",
    "## 1.7 生成模型与判别模型\n",
    "监督学习的任务就是学习一个模型，应用这一模型，对给定的输入预测相应的输出，这个模型的一般形式为决策函数 ： $Y=f(X)$ 或者条件概率分布 ：$P(Y|X)$ . <br>\n",
    "\n",
    "监督学习方法分类：\n",
    "1. 生成方法（generative apporach），学到的模型叫生成模型（generative model）\n",
    "2. 判别方法（discriminative apporach），学到的模型叫判别模型（discriminative model）\n",
    "\n",
    "生成模型：由数据学习联合概率分布$P(X,Y)$,然后求出条件概率分布$P(Y|X) = {P(X,Y) \\over P(X)} $ ,典型的生成模型：朴素贝叶斯和隐马尔可夫。 <br>\n",
    "判别模型：有数据直接学习决策函数$f(X)$或者条件分布概率$P(Y|X)作为预测模型。典型的判别模型：K近邻，感知机，逻辑回归，决策树等。<br>\n",
    "\n",
    "监督学习中生成方法和判别方法的优缺点：\n",
    "1. 生成方法：可以还原出联合概率分布 $P(X,Y)$;学习的收敛速度更快；存在引变量时仍可用生成方法学习。\n",
    "2. 判别方法：直接学习条件概率$P(X,Y)$或者决策函数。学习的准确率更高。\n",
    "   \n",
    "\n",
    "## 1.8 分类问题\n",
    "分类问题：输出变量$Y$是有限个离散的值的预测问题，输入变量$X$可以是离散的，也可以是连续的。<br>\n",
    "分类器（classifier）：从数据中学到的分类模型或分类决策函数。<br>\n",
    "分类：分类器对新的输入进行输出预测（prediction），称为分类（classification）。<br>\n",
    "\n",
    "分类问题的学习和分类过程：\n",
    "1. 学习过程：\n",
    "   根据已知训练集利用有效的学习方法学习一个分类器；<br>\n",
    "2. 分类过程：\n",
    "   分类系统通过学到分类器$P(Y|X)$ 或 $Y=f(X)$ 对新的输入实例$x_{N+1}$进行分类。<br>\n",
    "   <a href=\"https://imgse.com/i/piUorCT\"><img src=\"https://z1.ax1x.com/2023/11/20/piUorCT.png\" alt=\"piUorCT.png\" border=\"0\" /></a> <br>\n",
    "\n",
    "评估分类器性能的指标：\n",
    "1. 分类准确率(accuracy):对应给定的测试数据集，分类器正确分类的样本数和总样本数之比。也就是0-1损失时测试数据集上的准确率 （公式1.17）. <br>\n",
    "\n",
    "分类问题的常用指标：\n",
    "1. 精确率（precision）\n",
    "2. 召回率（recall）\n",
    "通常以关注的为正类，其他负类，则分类器在测试数据集上的预测结果又一下四种：\n",
    "1. TP-将正类预测为正类数\n",
    "2. FN-将正类预测负类数\n",
    "3. FP-将负类预测为正类数\n",
    "4. TN-将负类预测负类数\n",
    "\n",
    "精确率：$$ P={TP \\over (TP +FP)} （1.30）$$\n",
    "召回率：$$ R = {TP \\over (TP +FN)} （1.31）$$ \n",
    "\n",
    "此外，还有$F_1$ 值，是精确率和召回率的调和均值，$$ {2 \\over {F_1}} = {1 \\over P} + {1 \\over R} $$\n",
    "\n",
    "   \n",
    "## 1.9 标注问题\n",
    "标注问题可认为是分类问题的一个推广，又是复杂的结构预测（structure prediction）问题的简单形式。<br>\n",
    "标注问题数据的是一个观察序列，输出的是一个标记序列或状态序列。<br>\n",
    "\n",
    "标注问题的过程： 学习和标注。\n",
    "<a href=\"https://imgse.com/i/piUTwsH\"><img src=\"https://z1.ax1x.com/2023/11/20/piUTwsH.png\" alt=\"piUTwsH.png\" border=\"0\" /></a> <br>\n",
    "\n",
    "常用学习方法：隐马尔可模型，条件随机场。常用于信息提取、自然语言处理。\n",
    "\n",
    "## 1.10 回归问题\n",
    "回归用于预测输入变量（自变量）和输出变量（因变量）之间的关系，特别是输入变量的值发生变化时，输出的函数的值随之变化。<br>\n",
    "\n",
    "回归问题的分类：一元回归或多元回归；线性回归或分线性回归。<br>\n",
    "\n",
    "常用的损失函数:平方损失函数，可以用最小二乘法求解。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
