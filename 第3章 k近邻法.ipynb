{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第三章 k近邻法\n",
    "\n",
    "k近邻（k-nearest neighbor ，k-NN） --一种基本的分类与回归方法。<br>\n",
    "输入：实例的特征向量 <br>\n",
    "输出：实例的类别<br>\n",
    "\n",
    "通过多数表决等方式进行预测。因此k近邻不具有显式学习过程。k近邻利用训练数据集对特征向量空间进行划，并作为其分类的模型。<br>\n",
    "k近邻算法三要素：k值、距离度量及分类决策。<br>\n",
    "\n",
    "## 3.1 k近邻算法\n",
    "给定一个训练数据集，对于新的输入实例，在训练数据集中找到与该实例最邻近的k个实例，这k个实例的多数属于某个类，就把该输入实例分到这个类。<br>\n",
    "\n",
    "算法3.1(k近邻)<br>\n",
    "输入：训练数据集 $ T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_n,y_n)}$,其中，$x_i \\in \\chi \\supseteq R^n$ 为实例的特征向量，$y_i \\in \\upsilon  = {c_1,c_2,\\cdots,c_k}$ 为实例的类别，i = 1,2，...,N;实例特征向量x;<br>\n",
    "\n",
    "输出：实例x所属的类y.<br>\n",
    "1. 根据给定的距离度量，在训练集T中找到与x最近邻的k个点，涵盖这k个点的领域记作 $N_k(x)$;\n",
    "2. 在$N_k(x)$ 中根据分类决策规则（如多数表决）决定x的类别:\n",
    "   $$ y= arg \\underset {c_j} max  \\underset {x_j \\in N_k(x)} \\sum I(y_i = c_j),i = 1,2,\\cdots,N; j = 1,2,\\cdots,K (3.1) $$\n",
    "   式(3.1)中，I为指示函数，即当$y_i=c_j$时 I为1，否则I为0 。<br>\n",
    "\n",
    "k近邻的特殊情况是k = 1的情形，称为最近邻算法，对于输入的实例点（特征向量）x ,最近邻将训练数据集中与x最近邻的类作为x的类。\n",
    "\n",
    "## 3.2 模型\n",
    "模型三要素：距离度量、k值的选择和分类决策规则。\n",
    "\n",
    "### 3.2.1 模型\n",
    "k近邻算法中，当训练集、距离度量（如欧式距离）、k值及分类决策规则（如多数表决）确定后，对于任何一个新的输入实例，它所属的类唯一确定。这相当于根据上述要素将特征空间划分为一些子空间，确定子空间里每个点所属的分类。<br>\n",
    "\n",
    "特征空间，对每个训练实例点$x_i$ ,距离该点比其他点更近的所有点组成一个区域，叫做单元（cell），每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分<br>\n",
    "<a href=\"https://imgse.com/i/piDkAeg\"><img src=\"https://z1.ax1x.com/2023/11/28/piDkAeg.png\" alt=\"piDkAeg.png\" border=\"0\" /></a>\n",
    "\n",
    "\n",
    "### 3.2.2 距离度量\n",
    "特征空间中两个实例点的距离是相似程度的反映。 一般特征空间为n维实数向量空间$R^n$.<br>\n",
    "常用距离：\n",
    "1. 欧式距离\n",
    "2. $L_p$距离\n",
    "3. Minkowski距离\n",
    "   \n",
    "例：设特征空间$\\chi$ 是n维实数向量空间$R^n,x_i,x_j \\in \\chi ，x_i = ( x_i^{(1)},x_i^{(2)},\\cdots,x_i^{(n)})^T,x_j = ( x_j^{(1)},x_j^{(2)},\\cdots,x_j^{(n)})^T$,$x_i,x_j$的$L_p$距离定义为\n",
    "$$L_p(x_i,x_j) = \\left(  \\sum_{i=1}^{N} |x_i^{(l)} -x_j^{(l)}|^p \\right)^{\\frac{1}{p}}  (3.2)$$\n",
    "欧式距离：p= 2时 <br>\n",
    "曼哈顿距离： p = 1时 <br> \n",
    "p= ∞時 <br>\n",
    "\n",
    "图例：[![3.2](https://z1.ax1x.com/2023/11/28/piDk7kj.png)](https://imgse.com/i/piDk7kj)\n",
    "\n",
    "### 3.2.3 k值的选择\n",
    "应用中，k值一般取的比较小，然后才有交叉验证法莱选取最优的k值。\n",
    "\n",
    "### 3.2.4 分类决策规则\n",
    "一般常用多数表决，即有输入实例的K个临近的训练实例的多数类决定输入实例的类。<br>\n",
    "多数表决：<br> \n",
    "如果是0-1的分类损失函数，分类函数为：$f:R^n\\rightarrow{c_1,c_2,\\cdots,c_k}$,那么误分类的概率是 $P(Y \\neq f(X)) = 1-P(Y=f(X))$.<br>\n",
    "\n",
    "对给定的实力$x \\in \\chi$ ,其最近邻的k个训练实例点构成集合$N_k^{(x)}$ 如果涵盖$N_k^{x}$ 的的区域类是 $c_j$ ，那么误分类概率为\n",
    "$$ \\frac{1}{k} \\sum_{x_i \\in N_k^{x}} I(y_i \\neq c_j) = 1- I(y_i = c_j) $$\n",
    "\n",
    "## 3.3 K近邻的实现：kd树\n",
    "k近邻算法主要考虑的问题：如果快速进行k近邻搜索\n",
    "\n",
    "### 3.3.1 构造kd树\n",
    "kd树：一种树形结构（二叉树，可以对k维空间中的实例点进行存储和快速检索<br>\n",
    "\n",
    "构造方法：<br>\n",
    "1. 构造根节点，包含所有实例。\n",
    "2. 递归对K维空间进行切分，生成子节点。\n",
    "3. 子区域没有实例点石终止。\n",
    "\n",
    "通常，选择训练实例点在坐标轴的中数位对其切分，得到平衡kd树。<br>\n",
    "\n",
    "构造算法3.2：<br>\n",
    "输入：k维空间数据集$T={x_1,x_2,\\cdots,x_N}$ ,其中 $x_i = (x_i^{(1)},x_i^{(2)},\\cdots,x_i^{(k)})^T,i = 1,2,\\cdots,N;$ <br>\n",
    "输出：kd树。<br>\n",
    "\n",
    "1. 开始：选择$x^{(1)}$为坐标轴，以T中所有实例的$x^(1)$坐标的中位数为切分点，将根节点对应的超矩形区域划分成两个子区域。切分由通过切分点并于坐标轴$x^{(1)}垂直的超平面实现。\n",
    "2. 重复：对深度为j的结点，选择$x(l)$为切分的坐标轴，$ l = j(mod k) + 1 $\n",
    "3. 直到两个子区域内没有实例点停止。\n",
    "\n",
    "\n",
    "\n",
    "算法 3.3 （kd树的最近邻搜索）\n",
    "输入：已构造的kd树;目标x <br>\n",
    "输出：x的最近邻 <br>\n",
    "\n",
    "1. 在kd树中找到包含目标节点x的叶节点：从根节点出发，递归向下访问kd树。若目标点x当前维的坐标小于切分点的坐标，则移动到左子节点，否则移到右子节点。直到子节点为叶节点为止。\n",
    "2. 以此叶节点为“当前最近点”\n",
    "3. 递归向上回退.\n",
    "   1. 如果该节点保存的实例点比当前的最近点距离目标点更近，则以该实例点为最近点。\n",
    "   2. 检查另一子节点对应区域是否以目标点为球心，以目标点与“当前最近点”间的距离为半径的超球体相交。如果相交，可以在另一个区域存在更近的点，若不相交，向上回退。\n",
    "4. 当退回根节点时，搜索结束。\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = np.array(X)\n",
    "        self.y_train = np.array(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict(x) for x in X]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # 计算新样本与训练样本的欧氏距离\n",
    "        distances = [np.linalg.norm(x - x_train) for x_train in self.X_train]\n",
    "\n",
    "        # 找到距离最近的k个训练样本的索引\n",
    "        k_neighbors_indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "        # 获取对应的类别标签\n",
    "        k_neighbor_labels = [self.y_train[i] for i in k_neighbors_indices]\n",
    "\n",
    "        # 返回k个最近邻中最常见的类别作为预测结果\n",
    "        most_common = np.bincount(k_neighbor_labels).argmax()\n",
    "        return most_common\n",
    "\n",
    "# 示例数据集\n",
    "X_train = [[1, 2], [2, 3], [3, 4], [4, 5]]\n",
    "y_train = [0, 0, 1, 1]\n",
    "X_test = [[2, 3], [4, 5]]\n",
    "y_test = [0, 1]\n",
    "\n",
    "# 创建并训练KNN模型\n",
    "knn_model = KNNClassifier(k=3)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# 进行预测\n",
    "predictions = knn_model.predict(X_test)\n",
    "\n",
    "# 计算准确度\n",
    "accuracy = np.sum(predictions == y_test) / len(y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
