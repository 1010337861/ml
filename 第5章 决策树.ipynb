{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5章 决策树\n",
    "一种基本的树形分类与回归方法。<br>\n",
    "优点：具有可读性，分类速度快。<br>\n",
    "\n",
    "\n",
    "学习步骤：\n",
    "1. 特征选择\n",
    "2. 决策树的生成\n",
    "3. 决策树的修建\n",
    "\n",
    "\n",
    "## 5.1决策树模型与学习\n",
    "\n",
    "### 5.1.1 决策树模型\n",
    "\n",
    "分类决策树模型：描述对实例进行分类的树形结构<br>\n",
    "组成：节点（node）和有向边（directed edge）。<br>\n",
    "节点的类型：\n",
    "1.  内部节点：表示一个特征或属性\n",
    "2.  叶节点：表示一个类\n",
    "\n",
    "### 5.1.2 决策树与if-then规则\n",
    "决策树 <==> 一个if-then 规则的集合 <br>\n",
    "转换过程：由决策树的根节点到叶节点的每一条路径构建一条规则； 路径上内部节点的特征对应规则条件，而叶节点的类对应规则的结论<br>\n",
    "\n",
    "路径和规则集合的性质：完备并且互斥。\n",
    "每一个实例都被而且只被一条路径或规则覆盖。\n",
    "\n",
    "\n",
    "### 5.1.3 决策树与条件概率分布\n",
    "决策树 <==> 特征条件下类的条件概率分布\n",
    "\n",
    "\n",
    "\n",
    "### 5.1.4 决策树学习\n",
    "假定给定训练集 $$ D= {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)} $$ \n",
    "其中,$x_i = (x_i^{(1)},x_i^{(2)},\\cdots,x_i^{(n)}) $ 为输入实例（特征向量），n为特征个数，$ y \\in  {1,2,\\cdots,K}$ 为类标记， $i = 1,2,\\cdots,N$ ，N为样本容量。\n",
    "\n",
    "目标：根据给定训练集构建一个决策树模型，并能够正确分类。 <br>\n",
    "本质： 从训练数据集中归纳出一组分类规则。 <br>\n",
    "\n",
    "决策树学习的损失函数：通常为正则化的极大似然函数<br>\n",
    "\n",
    "决策树学习的算法:递归的选择最优特征，并根据该特征对训练数据进行分割,使对各个子数据集有个最好的分类过程。<br>\n",
    "\n",
    "## 5.2 特征选择\n",
    "### 5.2.1 特征选择问题\n",
    "选取目标：让学习器具有分类能力。<br>\n",
    "衡量准则：信息增益或信息增益比。<br>\n",
    "\n",
    "### 5.2.2 信息增益\n",
    "熵：表示随机变量不确定性的度量。<br>\n",
    "设X是一个取有限个值的随机变量，其条件概率分布 $$P(X=x_i) = p_i , i = 1,2,\\cdots,n $$ ，这随机变量X的熵定义为 $$ H(X) = - \\sum_{i= 1}^n p_i logp_i  \\tag{5.1} $$\n",
    "\n",
    "熵只依赖于X的分布，而与X的取值无关，即 X的熵也可表示成 $$ H(p) = - \\sum_{i= 1}^n p_i logp_i  \\tag{5.2} $$\n",
    "\n",
    "熵越大，随机变量的不确定性就越大\n",
    "$$ 0 \\leq H(p) \\leq logn  \\tag{5.3}$$\n",
    "\n",
    "当随机变量只取两个值是，例0，1 X 的分布 为 $ P(X=1) =p ,P(X=0) = 1-p ,0 \\leq p \\leq 1$ ,熵为$$H(p) = -plog_2p - (1-p)log_2(1-p) \\tag{5.4} $$\n",
    "\n",
    "当 p = 0 或 p = 1是 H(p) = 0 随机变量完全没有不确定性。 当p = 0.5是H(p) =  1 ,熵取值最大，随机变量不确定性最大。<br>\n",
    "\n",
    "条件熵 H(Y|X) : X给定条件下Y的条件概率分布的熵对X的数学期望 $$H(Y|X) = \\sum_{i=1}^n p_i H(Y|X=x_i) \\tag{5.5}$$\n",
    "$p_i=P(X=x_i),i = 1,2,\\cdots,n$   <br>\n",
    "\n",
    "当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所得到的被称为经验熵和条件经验熵。<br>\n",
    "\n",
    "信息增益： 表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。<br>\n",
    "特征A对训练集D的信息增益g(D,A) ,定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即$$ g(D,A) = H(D) - H(D|A) \\tag{5.6}$$\n",
    "\n",
    "互信息： 熵与条件熵之差 <br>\n",
    "\n",
    "根据信息增益选择特征选择的方法：对训练数据集（或）子集D,计算每个特征的信息增益，并比较大小，选择增益最大的特征 . <br>\n",
    "\n",
    "信息增益的算法：<br>\n",
    "输入：训练集D和特征A; <br>\n",
    "输出：特征A对训练数据集D的信息增益g(D,A)\n",
    "\n",
    "1. 计算数据集D的经验熵H(D)\n",
    "   $$ H(D) = =\\sum_{k=1}^k \\frac{|C_k|}{|D|} log_2 \\frac{|C_k|}{|D|} \\tag{5.7}$$\n",
    "2. 计算特征A对数据集D的经验条件熵H(D|A)\n",
    "    $$ H(D|A) = \\sum_{i=1}^n \\frac{|D_i|}{|D|} H(D_i) = =\\sum_{i-1}^n \\frac{|D_i|} {|D|} \\sum_{k=1}^K \\frac{|D_{ik}|}{|D_i|}log_2 \\frac{|D_ik|}{|D_i|} \\tag{5.8} $$\n",
    "3. 计算信息增益\n",
    "    $$ g(D,A) = H(D) - H(D|A) \\tag{5.9} $$\n",
    "\n",
    "### 5.2.3 信息增益比\n",
    "特征A对训练集D的信息增益比$g_R(D,A)$ 定义为其信息增益g(D,A) 与训练集D的经验熵H(D)之比： $$ g_R(D,A) = \\frac {g(D,A)} {H(D)} \\tag{5.10}$$ \n",
    "\n",
    "## 5.3 决策树的生成\n",
    "\n",
    "### 5.3.1 ID3算法\n",
    "核心：应用信息增益准则选择特征，递归构建决策树<br>\n",
    "具体方法：从根结点开始，对结点计算所有可能特征的信息增益，选择特征最大的信息增益的特征作为结点特征，由该特征的不同取值建立子结点；重复上述方法构建决策树。<br>\n",
    "\n",
    "算法：\n",
    "输入：训练集D ,特征集A,阈值$\\epsilon$ ;<br>\n",
    "输出：决策树T 。<br>\n",
    "1. 如果D中所有实例都属于同一类$C_k$ ,则T为单结点树，并将类$C_k$作为该节点的类标记，返回T;\n",
    "2. 如果A= $\\emptyset$,则T为单结点树，并将类$C_k$作为该节点的类标记，返回T;\n",
    "3. 如果不是上述情况，则根据信息增益的算法，选择信息增益的最大的特征$A_g$;\n",
    "4. 如果$A_g$的信息增益小于阈值$\\epsilon$ ，则置T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T;\n",
    "5. 否则，对$A_g$的每一个可能值$a_i$,依$A_g = a_i$ 将D分割成若干非空子集$D_i$,j将$D_i$中的实例数最大的类作为标记，构建子结点，有结点及其子结点构成数T,返回T;\n",
    "6. 对第i个子结点，以$D_i$为训练集，以$A- {A_g}$ 为特征集，递归调用1-5 得到子树 $T_i$ . \n",
    "   \n",
    "\n",
    "ID3 算法的缺点： 只有树的生成，容易产生过拟合。\n",
    "\n",
    "### 5.3.2 C4.5的生成算法\n",
    "\n",
    "使用信息增益比来进行选择特征。\n",
    "算法：\n",
    "输入：训练集D ,特征集A,阈值$\\epsilon$ ;<br>\n",
    "输出：决策树T 。<br>\n",
    "1. 如果D中所有实例都属于同一类$C_k$ ,则T为单结点树，并将类$C_k$作为该节点的类标记，返回T;\n",
    "2. 如果A= $\\emptyset$,则T为单结点树，并将类$C_k$作为该节点的类标记，返回T;\n",
    "3. 如果不是上述情况，则根据信息增益比的算法，选择信息增益的最大的特征$A_g$;\n",
    "4. 如果$A_g$的信息增益比小于阈值$\\epsilon$ ，则置T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T;\n",
    "5. 否则，对$A_g$的每一个可能值$a_i$,依$A_g = a_i$ 将D分割成若干非空子集$D_i$,j将$D_i$中的实例数最大的类作为标记，构建子结点，有结点及其子结点构成数T,返回T;\n",
    "6. 对第i个子结点，以$D_i$为训练集，以$A- {A_g}$ 为特征集，递归调用1-5 得到子树 $T_i$ . \n",
    "\n",
    "## 5.4 决策树的剪枝\n",
    "简化决策树，降低复杂度，避免过拟合<br>\n",
    "\n",
    "剪枝原理:通过极小化决策树整体的损失函数或代价函数来实现，设树T的叶结点个数为|T|,t是树T的叶结点，该叶结点有$N_t$个样本点，其中k类样本有$N_{tk}$个，$k = 1,2,3,...,K，H_t(t)$,为叶结点t上的经验熵， $\\alpha \\geq 0 $为参数，则决策树的损失函数可以定义为 $$C_a(T) = \\sum_{t=1}^{|T|} N_tH_t(T) + \\alpha |T| \\tag{5.11} $$\n",
    "其中经验熵为 $$ H_t(T) = -\\sum_{k} \\frac{N_{tk}}{\\N_t}  log \\frac{N_{tk}}{N_t} {N_{tk}}  \\tag{5.12}$$\n",
    "\n",
    "在损失函数中，将式(5.11),右端的第一项记作 $$ C(T) = \\sum_{t=1}^{|T|} N_tH_t(T) = -\\sum_{t=1}^{|T|}\\sum_{k=1}^{k} N_{tk} log\\frac{N_{tk}} {N_t} \\tag{5.13}$$\n",
    "\n",
    "这是有 $$ C_{\\alpha}(T) = C(T) +\\alpha|T|  \\tag{5.14}$$ \n",
    "1. $C(T)$ 表示模型对训练数据的预测误差，即训练模型与训练数据的拟合程度，|T|表示数据模型的复杂度，参数$\\alpha \\geq 0$ 控制两者之间的影响。\n",
    "2. 较大的 $\\alpha$ 促使选择较简单的模型（树），较小的 $\\alpha$ 促使选择较复杂的模型（树）。 等于0 时只考虑模型与训练数据的拟合程度，不考虑模型的复杂程度。\n",
    "   \n",
    "\n",
    "剪枝：当$\\alpha$ 确定时，选择损失函数最小的模型，即损失函数最小的子树。当$\\alpha$值确定，子树越大，往往与训练数据的拟合越好，但模型的复杂度也越高；<br>\n",
    "\n",
    "损失函数表示二者的平衡;<br>\n",
    "\n",
    "树剪枝的算法：<a href=\"https://imgse.com/i/pio7jDP\"><img src=\"https://s11.ax1x.com/2023/12/20/pio7jDP.png\" alt=\"pio7jDP.png\" border=\"0\" /></a>\n",
    "1. 输入：生成算法产生的整个树T,参数$\\alpha$.\n",
    "2. 输出：修剪后的子树$T_{\\alpha}$. <br>\n",
    "\n",
    "i. 计算每个结点的经验熵 <br>\n",
    "ii. 递归地从数的结点向上回缩。\n",
    "\n",
    "设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_B$与$T_A$,其对应的损失函数值分别是 $C_{\\alpha}(T_B) $和  $C_{\\alpha}(T_A)$ ,如果$$ C_{\\alpha}(T_A) \\leq C_{\\alpha}(T_B) \\tag{5.15}$$\n",
    "则进行剪枝，即将父结点变为新的叶结点。\n",
    "\n",
    "3. 返回2 直至不能继续为止，得到损失函数的最小子树 $T_{\\alpha}$\n",
    "   \n",
    "\n",
    "## 5.5 CART算法\n",
    "分类回归树。\n",
    "\n",
    "CART 算法：\n",
    "1. 决策树生成： 基于训练数据集生成决策树，生成的决策树要尽量大。\n",
    "2. 决策树剪枝： 用验证数据集对已生成的树进行剪枝并选择最优子树，这是损失函数最小作为剪枝标准。\n",
    "\n",
    "### 5.5.1 CART生成\n",
    "决策树的生成就是递归构建二叉决策树的过程。 对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择，生成二叉树。\n",
    "\n",
    "#### 1. 回归树的生成：\n",
    "给定训练数据集：$D={(x_1,y_1),(x_2,y_2),..,(x_N,y_N)} $ <br>\n",
    "回归树模型：$$f(x) = \\sum_{m=1}^M c_mI(x \\in R_m)  \\tag{5.16}$$\n",
    "\n",
    "预测误差：$ \\sum_{x_i \\in R_m} (y_i - f(x_i))^2$ 来表示。<br>\n",
    "\n",
    "$\\hat {c_m}$ 是$c_m$在单元$R_m$ 的最优值。 是$R_m$ 上所有输入实例$x_i$ 对应输出$y_i$的均值， 即$$ \\hat {c_m} = ave(y_i|x_i \\in R_m) \\tag{5.17}$$\n",
    "\n",
    "启发式方法：选择第j个变量 $x^(j)$ 和他取的值$ ,作为切分变量和切分点并定义两个区域：\n",
    "$$R_1(j,s) = {x|x^{(j)} \\leq s }   和   R_2(j,s) = {x|x^{(j)} > s } \\tag{5.18}$$ \n",
    "\n",
    "然后找最优的切分变量 j和 最优的切分点s， 具体的，求解 $$ min_{j,s} [min_{c_1} \\sum_{x_i \\in R_1{j,s}}(y_i - c_1)^2 + min_{c_2} \\sum_{x_i \\in R_2{j,s}}(y_i - c_2)^2] \\tag{5.19} $$\n",
    "\n",
    "\n",
    "对固定的输入变量j 可以找到最优切分点s\n",
    "\n",
    "$$\\hat{c_1} = ave(y_i|x_i \\in R_1(j,s))   和 \\hat{c_2} = ave(y_i|x_i \\in R_2(j,s)) \\tag{5.20} $$\n",
    "\n",
    "遍历所有的输入变量，找到最优的切分变量j, 构成一个对(j,s) 。依次将输入空间划分成两个区域，接着，对每个区域重复上述划分过程，直到满足条件为止。<br>\n",
    "\n",
    "算法5.5(最小而成回归树生成算法)\n",
    "1. 输入：训练数据集D;\n",
    "2. 输出：回归树f(x).\n",
    "\n",
    "在训练集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树；\n",
    "\n",
    "（1） 选择最优切分变量j与切分点s,求解 $$\\underset{j,s} {min}[\\underset{c_1}{min} \\underset{x_i \\in R_1(j,s) }{\\sum}(y_i -c_1)^2 + \\underset{c_2}{min} \\underset{x_i \\in R_2(j,s) }{\\sum}(y_i -c_2)^2]  \\tag{5.21}$$\n",
    "\n",
    "遍历变量j,对固定的切分变量j扫描切分点s，选择使式（5.21）达到最小值的对(j,s)。<br>\n",
    "\n",
    "(2) 用选定的对(j,s)划分区域并决定相应的输出值：$$R_1(j,s) = {x|x^{(j)} \\leq s} ,R_2(j,s) = {x|x^{(j)} > s} \\\\ \\hat{c_m}= \\frac{1}{N_m} \\underset{x_i \\in R_m(j,s)}{\\sum} y_i,x\\in R_m ,m=1,2 $$\n",
    "\n",
    "(3) 继续对两个子区域调用步骤（1），（2）。 直至满足停止条件。<br>\n",
    "\n",
    "(4) 将输入空间划分为M个区域$R_1,R_2,\\cdots,R_m $ 生成决策树： $f(x) = \\overset{M}{\\underset{m=1}{\\sum}} \\hat{c_m}I(x\\in R_m) $\n",
    "\n",
    "#### 2. 分类树的生成：\n",
    "分类树选取最优特征的依据：基尼指数,同时决定该特征的最优二值切分点 。<br>\n",
    "\n",
    "基尼指数：分类问题中，假设有K个类，样本点属于第k类的概率为$p_k$,这概率分布的基尼系数定义为 $$Gini(p) = \\overset{k}{\\underset{k=1}{\\sum}}p_k(1-p_k) = 1- \\overset{k}{\\underset{k=1}{\\sum}}p_k^2 \\tag{5.22}$$\n",
    "\n",
    "二分类:$$ Gini(p) = 2p(1-p) \\tag{5.23}$$\n",
    "\n",
    "给定样本集合D,其基尼指数为 $$ Gini(D) = 1- \\overset{k){\\underset{k=1}{\\sum}} (\\frac{|C_k|}{|D|})^2 \\tag{5.24}$$\n",
    "其中 C_k是D中属于第K类的样本集，K是类的个数。   <br>\n",
    "\n",
    "如果样本集合D 根据特征A 是否取某一可能值$ \\alpha $ 被分割成$D_1 和D_2$ 两个部分，即$ D_1 = {(x,y) \\ in D |A(x) = \\alpha} ,D_2 = D-D_1 $\n",
    "\n",
    "则这特征A的条件下，集合D的基尼指数定义为： $$Gini(D,A) = \\frac{|D1|}{|D|} Gini(D_1) +\\frac{|D2|}{|D|} Gini(D_2) \\tag{5.25} $$\n",
    "\n",
    "基尼指数Gini(D)表示集合D的不确定性，基尼指数Gini(D,A)表示经A=a 分割后集合D的不确定性，指数越大，不确定性就越大。<br>\n",
    "\n",
    "二分类中基尼指数、熵之半1/2H(P) 和分类误差的关系\n",
    "<a href=\"https://imgse.com/i/pi7r2Ix\"><img src=\"https://s11.ax1x.com/2023/12/23/pi7r2Ix.png\" alt=\"pi7r2Ix.png\" border=\"0\" /></a>\n",
    "\n",
    "\n",
    "#### CART生成算法\n",
    "输入：训练数据集D,停止计算的条件；<br>\n",
    "输出：CART决策数 <br>\n",
    "\n",
    "根据训练数据集，从根结点开始递归的对每个结点进行一下操作，构建二叉树:<br>\n",
    "1. 设几点的训练数据集为D,计算现有特征对该数据集的基尼指数。 此时，每个特征A，对其可能取的每个值a,根据样本点A= a 的测试为“是” 或 “否”将D 分割成$D_1$ 和$D_2$ 两个部分，利用（5.25）计算A=a是的基尼指数\n",
    "2. 在所有可能得特征A以及他们所有可能得切分点a中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。 以最优特征与最优切分点，从现结点生成两个子结点，将训练数据集以特征分配到两个子结点中去。\n",
    "3. 对两个子结点递归地调用（1）(2),直至满足停止条件\n",
    "4. 生成CART决策树。\n",
    "算法停止计算条件就是结点中的样本个数小于预定阈值，或者样本集中的基尼值数小于预定阈值（样本基本属于同一类） ，或者没有更多特征。\n",
    "\n",
    "### 5.5.2 CART剪枝\n",
    "算法组成：首先从生成算法产生的决策树$T_0$ 底端不断开始剪枝，直到$T_0$的根结点，形成一个子树序列${T_0,T_1,\\cdots,T_n}$,然后通过交叉验证在独立的验证数据集上进行测试，选择最优子树。<br>\n",
    "\n",
    "\n",
    "#### 剪枝算法：\n",
    "输入：CART算法生成的决策树$T_0$<br>\n",
    "输出：最优决策树$T_{\\alpha}<br>\n",
    "\n",
    "1. 设k=0 T = $T_0$ \n",
    "2. 设$\\alpha$ = +∞\n",
    "3. 自下而上地对各内部节点t计算$C(T_t)，|T_t|$,以及$$ g(t) = \\frac{C(t)-C(T_t)}{|T-t|-1} \\\\ \\alpha =min(\\alpha,g(t))$$\n",
    "$T_t$ 表示以t为根节点的子树，$C(T_t)$ 是对训练数据的预测误差，$|T_t|是$T_t$的叶结点个数\n",
    "4. 自下而上地访问内部结点t,如果有g(t) = $\\alpha$ ,记性剪枝，并对叶结点t以多数表决法决定其类，得到树T.\n",
    "5. 设K=k+1,$\\alpha_k = \\alpha ,T_k = T$ .\n",
    "6. 如果T不是由根结点单独构成的树，则回到步骤（4）\n",
    "7. 采用交叉验证法在子树序列 选取最优子树。 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
