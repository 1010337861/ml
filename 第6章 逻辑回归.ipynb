{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第6章 逻辑回归\n",
    "\n",
    "## 6.1 逻辑回归模型\n",
    "\n",
    "### 6.1.1 l 逻辑斯蒂分布\n",
    "\n",
    "逻辑斯蒂分布：设X是连续随机变量，X服从逻辑斯蒂分布是指X具有下列分布函数和目的函数：$$ F(X) = p(X \\leq x) = \\frac{1}{1+e^{\\frac{-(x-\\mu)}{\\gamma}}} \\tag{6.1}$$ \n",
    "$$ f(x) = F^{'}(x) = \\frac{e^{\\frac{-(x-\\mu)}{\\gamma}}}{\\gamma(1+e^{\\frac{-(x-\\mu)}{\\gamma}})^2} \\tag{6.2}$$\n",
    "其中$\\mu$为位置参数，$\\gamma >0 $ 为形状参数。 \n",
    "\n",
    "逻辑斯蒂分布的密度函数$f(x)$和分布函数$F(X)$: <br>\n",
    "<img src=\"https://s11.ax1x.com/2023/12/25/piHT9N8.png\" alt=\"piHT9N8.png\" border=\"0\" />\n",
    "\n",
    "分布函数属于逻辑斯蒂函数，该曲线以点$(\\mu,\\frac{1}{2})$ 为中心对称，既满足 $$F(-x+\\mu) -\\frac{1}{2} = -F(x-\\mu) + \\frac{1}{2}$$\n",
    "\n",
    "曲线在中心附近增长的较快，两端增长的慢。 形状参数$\\gamma$ 的值越小，曲线在中心附近增长的越快。\n",
    "\n",
    "逻辑斯谛函数（英语：logistic function）是一种常见的S型函数，其函数图像称为逻辑斯谛曲线（英语：logistic curve）\n",
    "\n",
    "### 6.1.2 二项逻辑斯蒂回归模型\n",
    "二项逻辑斯蒂回归模型（binomial logistic regression model）：一种分类模型，由条件概率分布$P(Y|X)$表示,形式为参数化的逻辑斯蒂分布。\n",
    "\n",
    "逻辑斯蒂回归模型：二项逻辑斯蒂回归模型的条件概率分布\n",
    "$$P(Y=1|x) = \\frac{exp(\\omega \\cdot x +b)} {1+exp(\\omega \\cdot x + b)}  \\tag{6.3}$$\n",
    "$$P(Y=0|x) = \\frac{1} {1+exp(\\omega \\cdot x + b)}  \\tag{6.4} $$\n",
    "$ x \\in R^n 输入，Y \\in {0,1} 输出$, $\\omega \\in R^n$ 和 $ b \\in R 是参数$ <br>\n",
    "$\\omega$：权重向量。<br>\n",
    "b：偏置 <br>\n",
    "$ \\omega \\cdot x $ 表示内积。<br>\n",
    "\n",
    "对于给定的实例x,根据式6.3 和 6.4 求得对应条件概率，比较条件概率值的大小， 将实例x分配到概率值较大的类别中。<br>\n",
    "\n",
    "简化：将权值向量和输入向量加以扩充，仍记作$\\omega,x$ 即 $\\omega = (\\omega^{(1)},\\omega^{(2)},\\cdots,\\omega^{(n)},b )^T ,x = (x^{(1)},x^{(2)},\\cdot,x^({n}),1)^T $，此时回归模型为：\n",
    "$$P(Y=1|x) = \\frac{exp(\\omega \\cdot x)} {1+ exp(\\omega \\cdot x)}  \\tag{6.5}$$\n",
    "$$P(Y=0|x) = \\frac{1} {1+ exp(\\omega \\cdot x)} \\tag{6.6}$$\n",
    "\n",
    "事件几率：指该事件发生的概率与不发生概率的比值。<br> \n",
    "比如一个事件发生的概率事p 则几率为$ \\frac{p}{1-p}$<br>\n",
    "该事件的对数几率或logit函数是 $logit(p) = log \\frac{p}{1-p} $,对于逻辑斯蒂回归来说，用式6.5 和 6.6 可得 $log\\frac{P(Y=1|x)}{1-p(Y=1|x)} = \\omega \\cdot x $ <br>\n",
    "含义：在逻辑斯蒂回归模型中，输出Y=1的对数几率是输入x的线性函数，或输出Y= 1 的对数几率是由输入x的线性函数表示的模型。<br>\n",
    "\n",
    "另一角度：对输入x进行分类的线性函数$\\omega \\cdot x$,其值域为实数域。 $ x \\in R^{n+1} ,\\omega \\in R^{n+1}$ 。通过式6.5 可以将线性函数 $ \\omega \\cdot x $ 转化为概率：\n",
    "$$P(Y=1|x) = \\frac{exp(\\omega \\cdot x)}{1+ exp(\\omega \\cdot x)}$$\n",
    "此时，线性函数的值越接近正无穷，概率值越接近1；越接近负无穷，概率值越接近0。<br>\n",
    "\n",
    "\n",
    "\n",
    "### 6.1.3 模型参数估计\n",
    "应用极大似然估计模型参数，得到逻辑斯蒂回归模型\n",
    "设：$P(Y=1|x) = \\pi(x) ,P(Y=0|x) =1-\\pi(x)$ ,似然函数为 $$\\overset{N}{\\underset{i=1}{\\prod}}[\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i} $$\n",
    "\n",
    "对数似然函数为 \n",
    "$$L(w) = \\overset{N}{\\underset{i=1}{\\sum}}[y_ilog \\pi(x_i)+(1-y_i)log(1-\\pi(x_i))] \\\\ =\\overset{N}{\\underset{i=1}{\\sum}}[y_ilog\\frac{\\pi(x_i)}{1-\\pi(x_i)} + log(1-\\pi(x_i))] \\\\ =\\overset{N}{\\underset{i=1}{\\sum}}[y_i(\\omega \\cdot x_i) - log(1+exp(\\omega \\cdot x_i))]  $$\n",
    "\n",
    "对$L(\\omega)$求极大值，得到$\\omega$的估计值. \n",
    "\n",
    "应用梯度下降及牛顿法求解对数似然函数最优化问题<br>\n",
    "\n",
    "假设w的极大似然估计值是$\\hat \\omega$,则学到的模型为，$$P=(Y=1|x) =\\frac{exp(\\hat{w} \\cdot x)}{1+exp(\\hat{w} \\cdot x}  \\\\ P=(Y=0|x) =\\frac{1}{1+exp(\\hat{w} \\cdot x}$$\n",
    "\n",
    "### 6.1.4 多项逻辑斯蒂回归\n",
    "多项式逻辑斯蒂回归模型 \n",
    "$$P(Y=k|x) = \\frac{exp(\\omega_k \\cdot x)}{1+ \\overset{k-1}{\\underset{k=1}{\\sum}}exp(\\omega_k \\cdot x)} , k =1,2,\\cdots , K-1 \\tag{6.7}$$\n",
    "$$P(Y=K|x) = \\frac{1}{1+ \\overset{k-1}{\\underset{k=1}{\\sum}}exp(\\omega_k \\cdot x)} \\tag{6.8}$$\n",
    "\n",
    "二项逻辑斯蒂回归的参数轨迹也可以推广到多项逻辑斯蒂回归。\n",
    "\n",
    "\n",
    "## 6.2 最大熵模型\n",
    "### 6.2.1 最大熵原理\n",
    "\n",
    "最大熵原理认为，学习概率模型时，在所有可能概率模型分布中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型的集合，所以最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。<br>\n",
    "\n",
    "### 6.2.2 最大熵模型定义\n",
    "假设分类模型是一个条件概率分布$P(Y|X)$ ,$ X \\in \\Chi  \\subseteq R^n$ 表示输入， $ Y \\in  \\Upsilon  \\subseteq R^n $,$\\Chi 和 \\Upsilon$ 表示输入和输出的集合。 <br>\n",
    "模型表示对于给定的输入X，以条件概率输出Y。<br>\n",
    "\n",
    "例：给定一个训练集 $T = {(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$ ,目标是用最大熵原理选择最好的分类模型。\n",
    "\n",
    "首先考虑模型应该满足的约束条件。<br>\n",
    "通过给定的训练集确定联合分布$P(X,Y)$的经验分布和边缘分布$P(X)$ 的经验分布，分别以 $\\widetilde{P}(X,Y) 和 \\widetilde{P}(X)$表示。<br>\n",
    "\n",
    "$$ \\widetilde{P}(X= x ,Y=y) = \\frac{v(X=x,Y=y)}{N}  $$\n",
    "$$ \\widetilde{p}(X=x) = \\frac{v(X=x)}{N} $$\n",
    "其中v(X=x,Y=y)表示训练数据样本(x,y)出现的频数，v(X=x)表示训练数据中输入x出现的频数，N表示训练样本容量。 <br>\n",
    "\n",
    "特征函数：f(x,y)描述输入x和输出y之间的某一个事实。 其定义是\n",
    "\n",
    "$$f(x)=\n",
    "\\begin{cases}\n",
    "1，& \\text{x与y满足某一事实}\\\\\n",
    "0，& \\text{否则}\n",
    "\\end{cases}$$\n",
    "\n",
    "这是二值函数，当x和y满足这个事实时取值1，否者取值0。 <br>\n",
    "\n",
    "特征函数f(x,y) 关于模型P(Y|X)与经验分布 $\\widetilde{p}(X)$的期望值，用$E_{p}(f)$表示。 \n",
    "$$E_{p}(f) = \\underset{x,y}{\\sum} \\widetilde{P}(x,y)f(x,y) $$ \n",
    "\n",
    "如果模型能够获取训练数据中的信息，那么就可以假设这两个期望值相等，即\n",
    "$$E_{p}(f) = E_{\\widetilde{p}}{f} \\tag{6.10}$$\n",
    "或\n",
    "$$\\underset{x,y}{\\sum}\\widetilde{p}(x)p{y|x}f(x,y) =\\underset{x,y}{\\sum} \\widetilde{p}(x,y)f(x,y)  \\tag{6,11}$$\n",
    "\n",
    "将式6.10和6.11 作为模型学习的约束条件，假如有n个特征函数$f_i(x,y) ,i=1,2,\\cdots,n $ ,那么就有n个约束条件。\n",
    "\n",
    "\n",
    "最大熵模型: 假设满足所有约束条件的模型集合为\n",
    "$$C ={P \\in \\Rho|E_{p}(f_i) = E_{\\widetilde{p}(f_i)},i =1,2,\\cdots,n}  \\tag{6.12} $$\n",
    "\n",
    "定义在条件概率分布P(Y|X)上的条件熵为 \n",
    "$$H(P) = - \\underset{x,y}{\\sum}\\widetilde{p}(x)P(y|x)logP(y|x) \\tag{6.13}$$\n",
    "模型集合C中条件熵H(P) 最大的模型称为最大熵模型，式中的对数为自然对数。\n",
    "\n",
    "### 6.2.3 最大熵模型的学习\n",
    "最大熵模型的学习过程就是求解最大熵模型的过程，最大熵模型的学习可以形式化为约束最优化问题。<br>\n",
    "\n",
    "按照最优化问题的习惯，将求最大值问题改写等价的求最小值问题：<br>\n",
    "$$ \\underset{p \\in C}{min} -H(P) = \\underset{x,y}{\\sum}\\widetilde{P}(x)P(y|x)logP(y|x)  \\tag{6.14}$$\n",
    "$$s.t.(受约束)  E_p(f_i) -E_{\\widetilde{p}}(f_i) = 0 , i = 1,2,\\cdots,n  \\tag{6.15}  $$\n",
    "$$ \\underset{y}{\\sum}P(y|x) = 1 \\tag{6.16} $$\n",
    "\n",
    "求解约束最优化问题6.14~6.16,所得解就是最大熵模型学习的解。<br>\n",
    "推导过程:首先，引进拉格朗日乘子$w_0,w_1,\\cdots,w_n$ 定义拉格朗日函数$L(P,\\omega)$:<br>\n",
    "$$L(P,\\omega) = -H(P) +w_0(1-\\underset{y}{\\sum}P(y|x)) + \\overset{n}{\\underset{i=1}{\\sum}}w_i(E_{\\widetilde{p}}(f_i) - E_p(fi))  \n",
    "\\\\ \\underset{x,y}{\\sum}\\widetilde{P}(x)P(y|x)logP(y|x) + w_0(1-\\underset{y}{\\sum}P(y|x)) + \\overset{n}{\\underset{i=1}{\\sum}}w_i(\\underset{x,y}{\\sum}\\widetilde{P}(x,y)f_i(x,y)-\\underset{x,y}{\\sum}\\widetilde{P}(x)P(y|x)f_i(x,y)) \\tag{6.17}\n",
    "$$\n",
    "\n",
    "最优化的原始问题\n",
    "$$ \\underset{p \\in C}{min} \\underset{\\omega}{max}L(P,\\omega) \\tag{6.18}$$\n",
    "对偶问题是\n",
    "$$\\underset{\\omega}{max} \\underset{p \\in C}{min} L(P,\\omega) \\tag{6.19}$$\n",
    "\n",
    "由于拉格朗日$L(P,\\omega)$是P的凸函数,原始问题6.18的解与6.19的解是等价的。<br> \n",
    "\n",
    "首先，求解对偶问题(6.19)内部的极小化问题 $\\underset{P \\in C}{min} L(P,w)$ . $$\\underset{P \\in C}{min} L(P,w)$ 是$\\omega$的函数，将其记作\n",
    "$$\\psi(\\omega) = \\underset{P \\in C}{min}L(P,\\omega) = L(P_{\\omega},w) \\tag{6.20} $$\n",
    "$\\psi(\\omega)$ 称为对偶函数，同时，将其解记作\n",
    "$$P_{\\omega} = arg \\underset{p \\in c}{min}L(P,w) =P_w(y|x)  \\tag{6.21} $$\n",
    "\n",
    "具体地，求$L(P,\\omega)$ 对P(y|x)的偏导数\n",
    "$$ \\frac{\\partial L(P,\\omega)}{\\partial P(y|x)} = \\underset{x,y}{\\sum} \\widetilde{P}(x)(logP(y|x)+1) - \\underset{y}{\\sum}w_0 - \\underset{x,y}{\\sum}(\\widetilde{p}(x)\\overset{n}{\\underset{i=1}{\\sum}}w_if_i(x,y)) \n",
    "\\\\ = \\underset{x,y}{\\sum}\\widetilde{P}(x) (logP(y|x) +1 -w_0 + \\overset{n}{\\underset{i=1}{\\sum}}w_if_i(x,y))  $$\n",
    "\n",
    "\n",
    "令偏导数等于0 ， 在 $ \\widetilde{P}(x) > 0  $的情况下，求解， \n",
    "$$ P(y|x) = exp(\\overset{n}{\\underset{i=1}{\\sum}}w_if_i(x,y) +w_0 -1) = \\frac{exp(\\overset{n}{\\underset{i=1}{\\sum}}w_if_i(x,y))}{exp(1-w_0)} $$\n",
    "\n",
    "由于 $ \\underset{y}{\\sum}P(y|x) = 1 $ 得<br>\n",
    "$$P_w(y|x) = \\frac{1}{Z_w(x)}exp(\\overset{n}{\\underset{i=1}{\\sum}}w_if_i(x,y))  \\tag{6.22}$$ \n",
    "其中，\n",
    "$$Z_w(x) = \\underset{y}{\\sum}exp(\\overset{n}{\\underset{i=1}{\\sum}}w_if_i(x,y))  \\tag{6.23}$$\n",
    "\n",
    "$Z_w(x)称为规范化因子$ ; $f_i(x,y)$ 是特征函数； $w_i$是特征权重。 <br> \n",
    "由是6.22、6.23 表示的模型 $P_w =P_w(y|x)$ 就是最大熵模型。这里w是最大熵模型中的参数向量。<br>\n",
    "\n",
    "之后，求解对偶问题外部的极大化问题 \n",
    "$$ \\underset{w}{max}\\psi(w) \\tag{6.24}$$\n",
    "将其解记为$w^{*}$  ，即\n",
    "$$w^{*} =arg \\underset{w}{max} \\psi(w) \\tag{6.25}$$\n",
    "\n",
    "### 6.2.4 极大似然估计\n",
    "\n",
    "已知训练函数的经验概率分布$\\widetilde{p}(X,Y)$ ,条件概率分布P(Y|X)的对数似然函数表示为\n",
    "$$L_{\\widetilde{p}}(P_w) = log \\underset{x,y}{\\prod}P(y|x)^{\\widetilde{p}(x,y)} = \\underset{x,y}{\\sum}\\widetilde{p}(x,y)logP(y|x) $$\n",
    "\n",
    "当条件概率分布P(y|x) 是最大熵模型6.22 和 6.23时，对数似然函数 $L_{\\widetilde{p}}(P_w)$ 为\n",
    "$$ L_{\\widetilde{p}}(P_w) = \\underset{x,y}{\\sum}\\widetilde{p}(x,y)logP(y|x)\n",
    "\\\\ = \\underset{x,y}{\\sum}\\widetilde{P}(x,y) \\overset{n}{\\underset{i=1}{\\sum}}w_if_i(x,y) - \\underset{x,y}{\\sum}\\widetilde{P}(x,y)logZ_w(x)\n",
    "\\\\ = \\underset{x,y}{\\sum}\\widetilde{P}(x,y) \\overset{n}{\\underset{i=1}{\\sum}}w_if_i(x,y) - \\underset{x}{\\sum}\\widetilde{P}(x)logZ_w(x) \\tag{6.26}\n",
    "$$\n",
    "\n",
    "偶函数 $\\psi(w)$ 由式6.17 和 6.20 可得\n",
    "$$ \\psi(w) = \\underset{x,y}{\\sum} \\widetilde{P}(x)P_w(y|x)logP_w(y|x) + \\overset{n}{\\underset{i=1}{\\sum}}w_i(\\underset{x,y}{\\sum}\\widetilde{P}(x,y)f_i(x,y) -\\underset{x,y}{\\sum}\\widetilde{P}(x)P_w{y|x}f_i(x,y)) \n",
    "\\\\ = \\underset{x,y}{\\sum}\\widetilde{P}(x,y) \\overset{n}{underset{i=1}{\\sum}}w_if_i(x,y) +   \\underset{x,y}{\\sum}\\widetilde{P}(x)P_w(y|x)(logP_w(y|x) - \\overset{n}{\\undeset{i=1}{\\sum}}w_if_i(x,y))\n",
    "\\\\ = \\underset{x,y}{\\sum}\\widetilde{P}(x,y) \\overset{n}{underset{i=1}{\\sum}}w_if_i(x,y) - \\underset{-x,y}{\\sum}\\widetilde{P}(x)P_w(y|x)logZ_w(x)\n",
    "\\\\ =  \\underset{x,y}{\\sum}\\widetilde{P}(x,y) \\overset{n}{underset{i=1}{\\sum}}w_if_i(x,y) - \\underset{x}{\\sum}\\widetilde{P}(x)logZ_w(x) \\tag{6.27}\n",
    "$$\n",
    "\n",
    "比较6.26和6.27可得 $\\psi(w) = L_{\\widetilde{p}}(P_w) $\n",
    "\n",
    "最大熵模型的一般形式：\n",
    "$$P_w(y|x) = \\frac{1}{Z_w(x)}exp(\\overset{n}{\\underset{i=1}{\\sum}}w_if_i(x,y)) \\tag{6.28}$$\n",
    "其中，\n",
    "$$Z_w(x) = \\underset{y}{\\sum}exp(\\overset{n}{\\underset{i=1}{\\sum}}w_if_i(x,y)) \\tag{6.29}$$\n",
    "\n",
    "## 6.3 模型学习的最优算法\n",
    "### 6.3.1 改进的迭代尺度法\n",
    "### 6.3.2 拟牛顿法\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
